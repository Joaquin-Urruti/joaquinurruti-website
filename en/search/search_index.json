{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"About Me","text":""},{"location":"#turn-your-data-and-ai-into-your-competitive-advantage","title":"Turn your data and AI into your competitive advantage","text":""},{"location":"#i-have-5-years-experience-doing-just-that","title":"I have 5+ years experience doing just that","text":"<ul> <li> <p>Are you struggling to keep up with the rapid pace of AI innovation?</p> </li> <li> <p>Do you need help translating AI hype into real business results?</p> </li> <li> <p>Need someone who understands both technical and business perspectives?</p> </li> <li> <p>Do you have lots of data but no insights?</p> </li> </ul> <p>Book Free Intro Call </p>"},{"location":"#about-me","title":"About me","text":"<p>Hi! I'm Joaqu\u00edn!</p> <p>I'm a GIS and AI consultant from Argentina. I work with agricultural companies and technical organizations facing the challenge of translating data capabilities and AI innovation into operational efficiency and competitive advantage.</p> <p>Most recently, I led the Innovation and Development department at Espartina S.A., an Argentinian agricultural production company with more than 2.000 fields and 150.000 hectares of land, driving its transformation toward an AI First model by leveraging technology, data, and artificial intelligence to create meaningful impact on decisions, processes, and people. We automated the analysis of new fields for leasing, scaled precision management zones from 50,000 to 150,000, achieved processing of 100% of yield monitor data, automated 100% of field trial processing, and implemented AI-driven automation to interpret the statistical analysis results of those trials.</p> <p>Before that, I led the GIS division at Espartina, developing tools and automating workflows for managing and analyzing spatial and productive data. Prior to that, I worked as a Senior Data Analyst for Major Accounts at S4 Agtech, a Buenos Aires\u2013based company specializing in parametric drought insurance for agriculture. In that position, I combined commercial and analytical responsibilities, helping clients understand their portfolio risk through satellite data and vegetation indices, and explaining how parametric insurance coverage could minimize those risks. We worked directly with re-insurers like Munich Re across Argentina, Uruguay, and Brazil, translating complex geospatial analytics into actionable risk management strategies.</p> <p>With a solid background in GIS, Python programming, Google Earth Engine, and PostGIS-backed database development, I design and implement innovation strategies that integrate geospatial data science, remote sensing, process automation, and AI-driven optimization. My focus is always on efficiency, scalability, and ensuring that solutions are truly adopted by the teams that use them.</p> <p>I'm an Agronomic Engineer from Buenos Aires University with a Master's in Open Source GIS and postgraduate studies in Agribusiness, Data Science, and Blockchain.</p> <p>Beyond agriculture and technology, I have explored the applications of blockchain and cryptocurrencies in fintech and ag-tech, and I maintain a strong creative side through music. I studied jazz trumpet for several years and performed as a soloist in different professional big bands and small jazz combos. I have also played the piano since the age of six, focusing on classical, romantic, and impressionist repertoire \u2014 an artistic journey that has deeply shaped my discipline, sensitivity, and creativity.</p>"},{"location":"#why-work-with-me","title":"Why work with me?","text":"<p>Here's what sets me apart and how I can help drive value for your business:</p> <ul> <li> <p> Proven Business Experience</p> <p>I come from a farming family and have an agronomy background and a career built around agricultural operations, I translate business needs into clear, actionable solutions, making the bridge between \u201cwhat the farm needs\u201d and \u201cwhat we build\u201d fast, natural, and frictionless.</p> </li> <li> <p> Educator &amp; Communicator</p> <p>My experience as a content creator and educator means I can break down complex technical concepts into clear, actionable insights. You'll always understand the 'why' behind technical decisions and get clear progress updates.</p> </li> <li> <p> Industry Expert</p> <p>With 15+ years in geospatial data and remote sensing, I help teams convert complex spatial and agronomic realities into reliable, actionable systems\u2014grounded in hands-on experience across agriculture, insurance, research NGOs, land valuation/division, and large-scale operations.</p> </li> <li> <p> Fast Implementation</p> <p>I specialize in rapid development and deployment of AI solutions. Using modern tools and proven frameworks, I can help you move from concept to production faster, giving you a competitive edge in today's fast-paced market.</p> </li> </ul>"},{"location":"#what-my-past-clients-say-about-my-work","title":"What my past clients say about my work","text":"<ul> <li> <p> Gabriel V\u00e1zquez Am\u00e1bile</p> <p>Agronomic Engineer (UBA), Ph.D. and Master of Science (Purdue University), Farm Administrator and Producer, Environmental Consultant at IPCC, PUMA, F.T di Tella, World Bank, FAO, AACREA and INTA.</p> <p>\"I am pleased to recommend Joaqu\u00edn Urruti as a highly skilled professional. As an Agricultural Engineer, he consistently leads in the adoption of advanced tools, technologies, and analytical approaches to address spatial and temporal variability in agricultural systems, delivering valuable, data-driven insights for decision-makers.</p> <p>Beyond his technical expertise, Joaqu\u00edn is a reliable and committed professional who works exceptionally well in teams. His proactive approach, sense of responsibility, and ability to generate synergy make him a valuable contributor to any collaborative environment.</p> <p>I strongly recommend Joaqu\u00edn to any organization or project seeking high-level expertise in geospatial analysis applied to agricultural production and land-use evaluation.\"</p> </li> <li> <p> Felipe Harrison</p> <p>Co-Founder at AgAnalyst Ltd.</p> <p>\"I\u2019ve had the pleasure of working with Joaqu\u00edn over the past two years, and I would especially highlight his high level of professionalism and rigorous approach to delivering technological solutions for operations. He is always attentive to new technologies and how to apply them to real-world use cases.</p> <p>Joaqu\u00edn stands out for his ability to design and implement robust, scalable systems aligned with business needs. He is a reliable and collaborative person, which makes working with him consistently a very positive experience. I recommend him without hesitation.\"</p> </li> <li> <p> Galbusera Sebastian</p> <p>Co-Founder and Chief Product Officer at Plataforma PUMA</p> <p>\"Joaqu\u00edn demonstrates excellent aptitude for developing innovative solutions, is proactive in information research and processing, designs effective technical communication tools, and works exceptionally well in collaborative environments.\"</p> </li> <li> <p> Francisco Lomazzi</p> <p>Sustentability Manager at Espartina S.A.</p> <p>\"Joaqu\u00edn is someone who is constantly looking to improve the things he does\u2014he doesn\u2019t settle for them simply working. I especially want to highlight his creativity, effort, and dedication, which drive him to pursue excellence continuously. </p> <p>We\u2019ve worked together on several sustainability-related projects, but I\u2019d like to single out one in particular: he created an automation to assess farms\u2019 legal compliance regarding agrochemical application distance requirements. It was a lot of work, but he delivered it successfully. </p> <p>A real pleasure to work with Joaqu\u00edn!\"</p> </li> <li> <p> Javier Moreira de Souza</p> <p>Specialist in Digital Agriculture | Agricultural Consultant.</p> <p>\"An intellectually sharp person, a lot of adrenaline when working together, very professional and pleasant, it was a pleasure to have worked together.\"</p> </li> </ul>"},{"location":"#frequently-asked-questions","title":"Frequently asked questions","text":"How quickly can you start working on my project? <p>I can typically begin new projects within 1-2 weeks of contract signing. For urgent matters, I maintain some flexibility for rapid response situations and can potentially start sooner - just let me know your timeline during our initial consultation.</p> Do you require a minimum project size or commitment? <p>While I can accommodate projects of any size, I find that engagements of at least 20 hours allow for meaningful impact. This gives us enough time to understand your data, implement solutions, and deliver actionable results. We can start with a small pilot project to ensure we're a good fit.</p> What industries do you have experience in? <p>I've successfully delivered projects across agriculture operations, administrations, farm valuations and parametric insurance services. While I specialize in AI, geospatial and remote sensing, I also apply my AI and python knowledge in projects involving all company areas.</p> How do you handle data security and confidentiality? <p>I take data security extremely seriously. I sign comprehensive NDAs before starting any project, use enterprise-grade encryption for all data transfers, and follow industry best practices for data handling. I can also work within your existing security infrastructure and policies.</p> What's your pricing structure? <p>I offer both project-based and retainer pricing models. Project fees are based on scope, complexity, and value delivered rather than hours worked. For ongoing support, I offer flexible retainer packages, and I can also provide ongoing support on an hourly basis when that\u2019s a better fit for your needs. Let\u2019s discuss your specific requirements during our consultation to determine the most cost-effective approach.</p> How do you communicate progress and results? <p>I maintain clear communication through weekly progress updates and regular check-in meetings. You'll receive detailed documentation of all analyses, findings, and recommendations.</p> <ul> <li> <p> Let's have a virtual coffee together!</p> <p>Want to see if we're a match? Let's have a chat and find out. Schedule a free 30-minute strategy session to discuss your challenges and explore how we can work together.</p> <p>Book Free Intro Call </p> </li> </ul>"},{"location":"blog/","title":"Blog","text":""},{"location":"blog/2026/01/03/running-uninterrupted-long-processes-with-macos-caffeinate-command/","title":"Running Uninterrupted Long Processes with MacOS \"caffeinate\" command","text":""},{"location":"blog/2026/01/03/running-uninterrupted-long-processes-with-macos-caffeinate-command/#why-use-the-caffeinate-command","title":"Why use the \"caffeinate\" command?","text":"<p>MacOS sleep mode interrupts long-running processes like GIS data analysis, satellite imagery processing, or database migrations. A 6-hour raster processing job can be killed when the system enters sleep mode, wasting hours of computation.</p>"},{"location":"blog/2026/01/03/running-uninterrupted-long-processes-with-macos-caffeinate-command/#solution-strategic-use-of-caffeinate","title":"Solution: Strategic Use of Caffeinate","text":"<p>The built-in <code>caffeinate</code> command prevents system sleep only when necessary, maintaining power efficiency while ensuring critical processes complete without interruption.</p>"},{"location":"blog/2026/01/03/running-uninterrupted-long-processes-with-macos-caffeinate-command/#how-caffeinate-works","title":"How Caffeinate Works","text":"<p><code>caffeinate</code> creates assertions that modify system sleep behavior. It operates in two modes:</p> <p>Wrapper Mode: When a program is specified, <code>caffeinate</code> creates assertions for that utility. Assertions remain active during execution and are automatically released upon completion.</p> <p>Direct Mode: Without a specified utility, <code>caffeinate</code> creates assertions directly and keeps them active until manually terminated (<code>Ctrl+C</code>) or the terminal closes.</p>"},{"location":"blog/2026/01/03/running-uninterrupted-long-processes-with-macos-caffeinate-command/#available-options","title":"Available Options","text":""},{"location":"blog/2026/01/03/running-uninterrupted-long-processes-with-macos-caffeinate-command/#-d-display-prevents-the-display-from-sleeping","title":"<code>-d</code> (display): Prevents the display from sleeping.","text":"<p>Use case: Visual process monitoring, presentations, real-time dashboards.</p>"},{"location":"blog/2026/01/03/running-uninterrupted-long-processes-with-macos-caffeinate-command/#-i-idle","title":"<code>-i</code> (idle)","text":"<p>Prevents the system from idle sleeping.</p> <p>Use case: Batch processes, analysis scripts, background operations without user interaction.</p>"},{"location":"blog/2026/01/03/running-uninterrupted-long-processes-with-macos-caffeinate-command/#-m-mediadisk","title":"<code>-m</code> (media/disk)","text":"<p>Prevents the disk from idle sleeping.</p> <p>Use case: Intensive I/O operations, database migrations, backups, continuous log writing.</p>"},{"location":"blog/2026/01/03/running-uninterrupted-long-processes-with-macos-caffeinate-command/#-s-system-on-ac","title":"<code>-s</code> (system on AC)","text":"<p>Prevents the entire system from sleeping.</p> <p>Important: Only valid when running on AC power. Ignored when on battery.</p> <p>Use case: Local development servers, critical processes requiring guaranteed continuous execution.</p>"},{"location":"blog/2026/01/03/running-uninterrupted-long-processes-with-macos-caffeinate-command/#-u-user-activity","title":"<code>-u</code> (user activity)","text":"<p>Declares that the user is active.</p> <p>Special behavior: - Automatically turns on the display if it's off - Prevents display from idle sleeping - Uses a default timeout of 5 seconds if <code>-t</code> is not specified</p> <p>Use case: Scripts requiring simulated user activity, keeping SSH sessions active.</p>"},{"location":"blog/2026/01/03/running-uninterrupted-long-processes-with-macos-caffeinate-command/#-t-seconds-timeout","title":"<code>-t</code> <code>&lt;seconds&gt;</code> (timeout)","text":"<p>Specifies the validity time of the assertion in seconds.</p> <p>Important: Not used in wrapper mode, as duration is controlled by the program's execution.</p> <p>Use case: Keep system awake for a specific known period (e.g., 3600 seconds = 1 hour).</p>"},{"location":"blog/2026/01/03/running-uninterrupted-long-processes-with-macos-caffeinate-command/#-w-pid-wait-for-process","title":"<code>-w</code> <code>&lt;pid&gt;</code> (wait for process)","text":"<p>Waits for the process with the specified PID to exit. Assertion is automatically released when the process terminates.</p> <p>Important: Ignored when used in wrapper mode.</p> <p>Use case: Keep system active while a specific running process continues working.</p>"},{"location":"blog/2026/01/03/running-uninterrupted-long-processes-with-macos-caffeinate-command/#common-flag-combinations-summary","title":"Common Flag Combinations Summary","text":"Flags Description Use Case <code>-i</code> Only prevents idle sleep Python scripts, batch processes <code>-di</code> Display + idle sleep Visual process monitoring <code>-ims</code> System + disk + idle (display OFF) Energy-efficient background processing <code>-dims</code> Full protection (display ON) Critical DB/disk operations with monitoring <code>-i -t 3600</code> Idle sleep for 1 hour Process with known duration <code>-u</code> Simulates user activity Keep sessions active"},{"location":"blog/2026/01/03/running-uninterrupted-long-processes-with-macos-caffeinate-command/#why-ims-is-recommended","title":"Why <code>-ims</code> is Recommended","text":"<p>The <code>-ims</code> combination is ideal for long-running background processes because:</p> <ul> <li>Energy efficient: Allows display to sleep, saving power</li> <li>System protection: Prevents system and disk idle sleep</li> <li>AC power awareness: The <code>-s</code> flag ensures protection only when plugged in</li> <li>Perfect for overnight jobs: GIS processing, database operations, data pipelines</li> </ul>"},{"location":"blog/2026/01/03/running-uninterrupted-long-processes-with-macos-caffeinate-command/#implementation-examples","title":"Implementation Examples","text":""},{"location":"blog/2026/01/03/running-uninterrupted-long-processes-with-macos-caffeinate-command/#basic-process-wrapping","title":"Basic Process Wrapping","text":"<pre><code>caffeinate -i python analyze_ndvi_timeseries.py\n</code></pre>"},{"location":"blog/2026/01/03/running-uninterrupted-long-processes-with-macos-caffeinate-command/#time-based-execution","title":"Time-Based Execution","text":"<pre><code># Keep system awake for 8 hours\ncaffeinate -i -t 28800\n</code></pre>"},{"location":"blog/2026/01/03/running-uninterrupted-long-processes-with-macos-caffeinate-command/#energy-efficient-background-processing","title":"Energy-Efficient Background Processing","text":"<pre><code>caffeinate -ims python batch_processing_overnight.py\n</code></pre>"},{"location":"blog/2026/01/03/running-uninterrupted-long-processes-with-macos-caffeinate-command/#critical-operations-with-monitoring","title":"Critical Operations with Monitoring","text":"<pre><code>caffeinate -dims python process_data.py\n</code></pre>"},{"location":"blog/2026/01/03/running-uninterrupted-long-processes-with-macos-caffeinate-command/#database-operations","title":"Database Operations","text":"<pre><code>caffeinate -dims psql -f migration_script.sql\n</code></pre>"},{"location":"blog/2026/01/03/running-uninterrupted-long-processes-with-macos-caffeinate-command/#monitor-existing-process","title":"Monitor Existing Process","text":"<pre><code>caffeinate -i -w 12345\n</code></pre>"},{"location":"blog/2026/01/03/running-uninterrupted-long-processes-with-macos-caffeinate-command/#automation-pipeline-integration","title":"Automation Pipeline Integration","text":"<pre><code>#!/bin/bash\n# run_geo_pipeline.sh\n\ncaffeinate -ims python preprocess_sentinel2.py &amp;&amp; \\\npython train_crop_classifier.py &amp;&amp; \\\npython generate_yield_predictions.py\n</code></pre>"},{"location":"blog/2026/01/03/running-uninterrupted-long-processes-with-macos-caffeinate-command/#results","title":"Results","text":"<ul> <li>Zero interrupted processes: Reliable completion of overnight jobs</li> <li>Improved resource utilization: Systems sleep when idle, saving energy</li> <li>Better debugging: Display stays active for monitoring when needed</li> <li>Simplified deployment: Single command handles sleep management ```</li> </ul>"},{"location":"blog/2026/01/03/running-uninterrupted-long-processes-with-macos-caffeinate-command/#working-with-similar-geospatial-data-workflows","title":"Working with similar geospatial data workflows?","text":"<p>If you're dealing with interrupted processing pipelines or need help optimizing your GIS automation infrastructure, let's connect. I specialize in building robust data processing systems for agricultural and environmental applications.</p> <p>Book Free Intro Call </p>"},{"location":"portfolio/","title":"Featured Projects","text":"<p>Welcome to my portfolio of geospatial data science and AI projects. Each project demonstrates my expertise in delivering impactful solutions to real-world business challenges.</p> <ul> <li> <p>Automatic phytosanitary application compliance mapping</p> <p>Automated GIS system for calculating phytosanitary application restriction zones across Argentine agricultural fields, processing multi-jurisdictional regulations to generate compliance maps and reports for aerial and terrestrial spraying operations.</p> </li> <li> <p>Large-Scale Distance Matrices with Local OSRM Server</p> <p>Running OSRM locally enables fast, unlimited calculation of large origin\u2013destination distance matrices using real road networks. This approach removes public API limits and helps companies model logistics costs accurately by computing kilometers between thousands of origins and destinations.</p> </li> </ul>"},{"location":"portfolio/projects/project-1/","title":"Automatic crop protection compliance mapping across Argentina","text":"<p>Case Study Summary</p> <p>Client: Argentine Agricultural Production Company Industry: Agricultural Technology / Regulatory Compliance  </p> <p>Impact Metrics:</p> <ul> <li>1000x reduction in manual GIS processing time per campaign</li> <li>+2100 agricultural lots analyzed across Argentina per processing cycle</li> <li>+70 departments/partidos with legislation tracked in the compliance database</li> <li>100% compliance with local phytosanitary regulations across all jurisdictions</li> <li>+100 of analyst time saved per agricultural campaign</li> </ul>"},{"location":"portfolio/projects/project-1/#end-to-end-automation-of-phytosanitary-applications-compliance-mapping-across-argentina-with-an-automated-gis-workflow","title":"End-to-end automation of phytosanitary applications compliance mapping across Argentina with an automated GIS workflow","text":""},{"location":"portfolio/projects/project-1/#overview","title":"Overview","text":"<p>Development of an automated geospatial compliance system that calculates exclusion and buffer zones for phytosanitary product applications across agricultural fields in Argentina. The system processes multiple regulatory frameworks from different jurisdictions to generate precise restriction maps and compliance reports for both aerial and terrestrial applications.</p>"},{"location":"portfolio/projects/project-1/#the-challenge","title":"The Challenge","text":"<p>Agricultural operations in Argentina must comply with strict regulations regarding the application of phytosanitary products near sensitive areas such as schools, urban zones, water bodies, and tree lines. The complexity arises from three key factors:</p> <p>Jurisdictional Fragmentation: Each state ('partido or departamento') in Argentina has autonomous authority to define its own restriction distances, resulting in a patchwork of regulations across the country's agricultural regions.</p> <p>Multiple Restriction Types: Regulations distinguish between two application methods (aerial and terrestrial) and two restriction levels (total exclusion zones where no application is permitted, and buffer zones where only green-band products are allowed).</p> <p>Operational Scale: The company manages agricultural fields distributed across the entire Argentine agricultural belt, requiring compliance verification for dozens of jurisdictions with different regulatory requirements each growing season.</p> <p>Previously, this analysis required a lot of manual GIS work for each department\u2014creating individual buffers around sensitive objects using jurisdiction-specific distances, then intersecting these with field boundaries. This process was time-consuming, error-prone, and difficult to delegate to non-GIS specialists.</p>"},{"location":"portfolio/projects/project-1/#technical-approach","title":"Technical Approach","text":""},{"location":"portfolio/projects/project-1/#technology-stack","title":"Technology Stack","text":"<ul> <li>Programming Language: Python 3.x</li> <li>Geospatial Processing: GeoPandas, Shapely</li> <li>Development Environment: Google Colab (Jupyter Notebook)</li> <li>Data Storage: Google Drive</li> <li>Data Formats: GeoPackage (.gpkg), Excel (.xlsx)</li> <li>Coordinate Reference System: EPSG:32720 (UTM Zone 20S)</li> </ul>"},{"location":"portfolio/projects/project-1/#architecture","title":"Architecture","text":"<p>!!! info \"System Architecture\" The solution follows a batch processing architecture with cloud-based storage and execution.</p> <pre><code>**Components**:\n\n- **Input Layer**: Google Drive folder containing standardized GeoPackage files\n- **Processing Engine**: Python notebook executing spatial operations\n- **Legislation Database**: GeoPackage with regulatory matrix by jurisdiction\n- **Output Generator**: Automated export of processed layers and reports\n</code></pre>"},{"location":"portfolio/projects/project-1/#data-model","title":"Data Model","text":"<p>The legislation database maintains a regulatory matrix with 15 distance parameters per department:</p> Parameter Type Objects Covered Application Types Exclusion distances Trees, Urban areas, Schools, Water courses, Water bodies Terrestrial, Aerial Buffer distances Trees, Urban areas, Schools, Water courses, Water bodies Terrestrial, Aerial"},{"location":"portfolio/projects/project-1/#implementation-highlights","title":"Implementation Highlights","text":""},{"location":"portfolio/projects/project-1/#regulatory-matrix-management","title":"Regulatory Matrix Management","text":"<p>The system ingests a legislation layer where each department contains codified distance values following a standardized naming convention:</p> <pre><code>DISTANCE_CODES = {\n    'EPT': 'Exclusion - Urban Areas - Terrestrial',\n    'APT': 'Buffer - Urban Areas - Terrestrial',\n    'EPA': 'Exclusion - Urban Areas - Aerial',\n    'APA': 'Buffer - Urban Areas - Aerial',\n    'EET': 'Exclusion - Schools - Terrestrial',\n    'AET': 'Buffer - Schools - Terrestrial',\n    # Additional codes for water bodies and water courses\n}\n</code></pre>"},{"location":"portfolio/projects/project-1/#dynamic-buffer-generation","title":"Dynamic Buffer Generation","text":"<p>A key technical challenge was handling the cumulative nature of buffer zones. The buffer distance is measured from the sensitive object, not from the exclusion zone boundary. The solution adjusts buffer distances automatically</p>"},{"location":"portfolio/projects/project-1/#spatial-processing-pipeline","title":"Spatial Processing Pipeline","text":"<p>The core processing follows a systematic approach:</p> <ol> <li>Geometry Repair: Input layers are cleaned to fix geometry and topology errors</li> <li>Reprojection: All layers converted to UTM Zone 20S for metric calculations</li> <li>Spatial Join: Sensitive objects inherit regulatory parameters from their containing department</li> <li>Buffer Generation: 15 distinct buffer layers created based on object type and restriction category</li> <li>Intersection: Buffers clipped to agricultural lot boundaries</li> <li>Overlap Resolution: Buffer zones have exclusion zones subtracted to prevent double-counting</li> </ol> <p></p> <p>Diagram of the geospatial processing workflow for exclusion and buffer zones: from regulatory and geographic data ingestion to the generation of non-overlapping buffers by restriction type, clipped to agricultural plot boundaries.</p> <p> Exclusion and buffer zone for terrestrial applications in a given field.</p>"},{"location":"portfolio/projects/project-1/#results-impact","title":"Results &amp; Impact","text":"<p>The automated system delivers significant operational improvements:</p> Metric Result Processing time reduction 99% compared to manual workflow - from 100 hours to les than 10 minutes Jurisdictions covered +300 departments across Argentina Accuracy 100% regulatory compliance with local ordinances Report generation Automated Excel and GeoPackage outputs User accessibility Non-GIS specialists can execute the workflow <p>Business Benefits:</p> <ul> <li>Pre-lease Negotiation: Restriction data available before field rental decisions, enabling informed negotiations</li> <li>Regulatory Compliance: Complete documentation of restricted areas for audit purposes</li> <li>Strategic Analysis: Aggregated metrics by zone, department, or restriction type for management reporting</li> <li>Operational Planning: Clear delineation of areas requiring special treatment or product restrictions</li> </ul>"},{"location":"portfolio/projects/project-1/#my-contributions","title":"My Contributions","text":"<p>As the sole developer of this solution, my responsibilities included:</p> <ul> <li>Requirements Analysis: Translated complex regulatory requirements into a structured data model</li> <li>Architecture Design: Designed the cloud-based workflow optimized for delegation to non-technical users</li> <li>Geospatial Development: Implemented all spatial processing algorithms including buffer generation, intersection, and overlap resolution</li> <li>Documentation: Produced comprehensive technical documentation for maintenance and knowledge transfer</li> </ul>"},{"location":"portfolio/projects/project-1/#lessons-learned","title":"Lessons Learned","text":"<p>Simplicity Enables Adoption: By designing the solution as a Colab notebook with Google Drive integration, the workflow could be delegated to team members without GIS expertise. The decision to prioritize usability over technical sophistication proved critical for operational success.</p> <p>Regulatory Data Maintenance: The most challenging aspect is keeping the legislation database current. Establishing a clear process for monitoring regulatory changes and updating the database is as important as the technical implementation.</p> <p>Geometric Precision Matters: Agricultural compliance requires precise area calculations. Investing time in geometry repair and proper coordinate system selection prevented downstream issues with area discrepancies.</p> <ul> <li> <p> Let's have a virtual coffee together!</p> <p>Want to see if we're a match? Let's have a chat and find out. Schedule a free 30-minute strategy session to discuss your challenges and explore how we can work together.</p> <p>Book Free Intro Call </p> </li> </ul>"},{"location":"portfolio/projects/project-2/","title":"Computing Large-Scale Distance Matrices with a Local OSRM Server","text":"<p>Project Summary</p> <p>Project Type: Open Source Tool / Internal Infrastructure Repository: osrm-local-server Industry: Logistics / Supply Chain / Agricultural Distribution  </p> <p>Key Outcomes:</p> <ul> <li>Unlimited distance calculations without API rate limits</li> <li>Very fast matrix generation for large origin-destination pairs</li> <li>100% cost elimination vs. commercial routing API usage</li> <li>Full control over data freshness and routing parameters</li> <li>Repeatable, auditable distance metrics for compliance</li> <li>Scalable to regional or country-level logistics networks</li> </ul> <p>This project addresses a common bottleneck in logistics operations: calculating road distances between many locations efficiently. When public routing APIs become too slow, expensive, or rate-limited, running OSRM locally transforms routing into a predictable, high-throughput service that scales with your business needs.</p>"},{"location":"portfolio/projects/project-2/#do-you-need-to-calculate-transport-costs-based-on-a-very-large-distance-matrix","title":"Do you need to calculate transport costs based on a very large distance matrix?","text":"<p>When you\u2019re pricing logistics, kilometers are money. If your business needs to compute road distance between many origins (warehouses, farms, stores, supplier locations) and many destinations (customers, ports, plants, distribution centers), the usual \u201ccall a routing API per pair\u201d approach falls apart quickly: it\u2019s slow, rate-limited, and expensive.</p> <p>This project provides a practical alternative: run OSRM locally and generate large origin\u2013destination (OD) distance matrices quickly, using OSRM\u2019s Table Service instead of individual route calls.</p> <p>The result is a system designed for companies that need to calculate logistics costs at scale, without depending on the public OSRM API or its usage limits.</p>"},{"location":"portfolio/projects/project-2/#why-this-matters-for-cost-modeling","title":"Why this matters for cost modeling","text":"<p>In most logistics and supply-chain models, distance is a core input:</p> <ul> <li>Transport cost is usually proportional to kilometers traveled.</li> <li>Choosing the optimal destination (plant, port, DC) depends on relative distances.</li> <li>Costs must be recalculated frequently as volumes, routes, or commercial conditions change.</li> </ul> <p>Public routing APIs are not designed for these workloads. Rate limits, request caps, and latency make them unsuitable for building large OD matrices repeatedly.</p> <p>Running OSRM locally removes these constraints and turns routing into a predictable, high-throughput internal service.</p>"},{"location":"portfolio/projects/project-2/#what-the-project-delivers","title":"What the project delivers","text":"<p>This repository implements an end-to-end workflow to compute distance matrices using a local OSRM instance:</p> <ol> <li>Start an OSRM server locally using Docker and OpenStreetMap data.</li> <li>Load origin geometries (polygons) and destination points from GIS files.</li> <li>Generate centroids for origin polygons.</li> <li>Snap centroids to the nearest road segment to ensure routable points.</li> <li>Use OSRM\u2019s Table Service to compute all origin\u2013destination distances and durations in one request.</li> <li>Export the results to Excel for further analysis or cost modeling.</li> </ol> <p>The example configuration is set up for Argentina, but the same approach works for any region with available OpenStreetMap extracts.</p>"},{"location":"portfolio/projects/project-2/#key-architectural-decisions","title":"Key architectural decisions","text":""},{"location":"portfolio/projects/project-2/#osrm-running-locally-in-docker","title":"OSRM running locally in Docker","text":"<p>OSRM is a high-performance routing engine built on OpenStreetMap data. Running it locally gives full control over:</p> <ul> <li>Data freshness</li> <li>Compute resources</li> <li>Request volume</li> <li>Reproducibility of results</li> </ul> <p>Docker ensures the setup is repeatable and easy to deploy across environments.</p>"},{"location":"portfolio/projects/project-2/#using-the-table-service-for-matrices","title":"Using the Table Service for matrices","text":"<p>Instead of computing one route at a time, the project uses OSRM\u2019s Table Service, which computes a full distance and duration matrix between multiple sources and destinations in a single call.</p> <p>This approach drastically reduces overhead and makes large matrices feasible.</p>"},{"location":"portfolio/projects/project-2/#snapping-origins-to-the-road-network","title":"Snapping origins to the road network","text":"<p>Origin data often comes as polygons (fields, zones, service areas). Their centroids may not lie exactly on a routable road.</p> <p>The pipeline snaps each centroid to the nearest road node before routing, ensuring realistic and reliable distance calculations.</p> <p> Schema of the workflow</p>"},{"location":"portfolio/projects/project-2/#how-to-run-the-project","title":"How to run the project","text":""},{"location":"portfolio/projects/project-2/#requirements","title":"Requirements","text":"<ul> <li>Docker</li> <li>Python 3.11+</li> <li>Jupyter Notebook</li> <li>Dependencies installed via <code>uv</code> or <code>pip</code></li> </ul>"},{"location":"portfolio/projects/project-2/#start-the-local-osrm-server","title":"Start the local OSRM server","text":"<pre><code>git clone https://github.com/Joaquin-Urruti/osrm-local-server\ncd osrm-local-server\n\nuv sync\n# or\npip install -r requirements.txt\n\n./iniciar_server_osrm_docker.sh\n</code></pre> <p>The first run preprocesses the OpenStreetMap extract and takes longer. Subsequent runs reuse the processed data and start quickly.</p>"},{"location":"portfolio/projects/project-2/#input-data","title":"Input data","text":""},{"location":"portfolio/projects/project-2/#place-your-gis-files-under-inputs","title":"Place your GIS files under inputs:","text":"<ul> <li>\"origins.gpkg\" or .shp: polygon geometries with identifiers (in this project polygons were used, but they could be points). </li> <li>\"destinations.gpkg\" or .shp: point geometries for destinations.  </li> </ul> <p>All data is converted internally to WGS84 (EPSG:4326) for compatibility with OSRM.</p>"},{"location":"portfolio/projects/project-2/#generate-the-distance-matrix","title":"Generate the distance matrix","text":""},{"location":"portfolio/projects/project-2/#run-the-notebook-distancias_tableipynb-it-will","title":"Run the notebook distancias_table.ipynb. It will:","text":"<ul> <li>Filter origins by campaign and optional zones</li> <li>Generate and snap centroids</li> <li>Query the local OSRM Table Service</li> <li>Export the OD matrix to: outputs/matrix.xlsx</li> </ul> <p>Each row represents an origin\u2013destination pair with distance (km) and duration (hours).</p>"},{"location":"portfolio/projects/project-2/#results-and-business-impact","title":"Results and business impact","text":"<p>The main result is the ability to compute large distance matrices quickly and reliably, without depending on the public OSRM server.</p>"},{"location":"portfolio/projects/project-2/#for-companies-this-enables","title":"For companies, this enables:","text":"<ul> <li>Faster logistics cost calculations</li> <li>Repeatable and auditable distance metrics</li> <li>Scenario analysis across many origins and destinations</li> <li>Full control over routing data and performance</li> </ul> <p>This setup is particularly valuable for agricultural logistics, retail distribution, manufacturing supply chains, and any operation where kilometers directly affect margins.</p>"},{"location":"portfolio/projects/project-2/#in-summary","title":"In summary","text":"<p>If your organization needs to calculate logistics costs at scale and public routing APIs are becoming a bottleneck, running OSRM locally with the Table Service is a solid, production-ready approach.</p> <p>This project provides a concrete, reusable starting point for building those distance matrices efficiently and without external limits.</p> <ul> <li> <p> Let's have a virtual coffee together!</p> <p>Want to see if we're a match? Let's have a chat and find out. Schedule a free 30-minute strategy session to discuss your challenges and explore how we can work together.</p> <p>Book Free Intro Call </p> </li> </ul>"},{"location":"blog/archive/2026/","title":"2026","text":""},{"location":"blog/category/tools/","title":"Tools","text":""},{"location":"blog/category/quick-tips/","title":"Quick Tips","text":""},{"location":"blog/category/commands/","title":"Commands","text":""}]}