{"config":{"lang":["es"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Sobre m\u00ed","text":""},{"location":"#transforma-tus-datos-e-ia-en-tu-ventaja-competitiva","title":"Transform\u00e1 tus datos e IA en tu ventaja competitiva","text":"<p>Tengo m\u00e1s de 15 a\u00f1os trabajando con sistemas GIS y Teledetecci\u00f3n, y m\u00e1s de 5 a\u00f1os implementando soluciones de IA y datos</p> <ul> <li> <p>\u00bfTe cuesta mantenerte al d\u00eda con el ritmo acelerado de la innovaci\u00f3n en IA?</p> </li> <li> <p>\u00bfNecesit\u00e1s ayuda para traducir el hype de la IA en resultados de negocio reales?</p> </li> <li> <p>\u00bfNecesit\u00e1s alguien que entienda tanto la perspectiva t\u00e9cnica como la del negocio?</p> </li> <li> <p>\u00bfTen\u00e9s muchos datos pero no les est\u00e1s sacando provecho?</p> </li> </ul> <p>Agend\u00e1 una llamada gratuita </p>"},{"location":"#sobre-mi","title":"Sobre m\u00ed","text":"<p>\u00a1Hola! \u00a1Soy Joaqu\u00edn!</p> <p>Soy consultor de datos geoespaciales, GIS e IA de Argentina. Trabajo con empresas agr\u00edcolas, start-ups y organizaciones t\u00e9cnicas que enfrentan el desaf\u00edo de traducir capacidades de datos e innovaci\u00f3n en IA en eficiencia operativa y ventaja competitiva.</p> <p>Recientemente lider\u00e9 el departamento de Innovaci\u00f3n y Desarrollo en Espartina S.A., una empresa argentina de producci\u00f3n agr\u00edcola con m\u00e1s de 2.000 lotes y 150.000 hect\u00e1reas de tierra, impulsando su transformaci\u00f3n hacia un modelo de m\u00e1xima eficiencia, apalancado en la tecnolog\u00eda, los datos y la inteligencia artificial, para crear un impacto significativo en las decisiones, los procesos y las personas. Automatizamos el an\u00e1lisis de nuevos campos para arrendamiento, escalamos las zonas de manejo de precisi\u00f3n de 50.000 a 150.000 hect\u00e1reas, logramos procesar el 100% de los datos de monitores de rendimiento, automatizamos el 100% del procesamiento de ensayos de campo e implementamos automatizaci\u00f3n impulsada por IA para interpretar los resultados del an\u00e1lisis estad\u00edstico de esos ensayos.</p> <p></p> <p>Antes de eso, lider\u00e9 la divisi\u00f3n GIS en Espartina, desarrollando herramientas y automatizando flujos de trabajo para gestionar y analizar datos espaciales y productivos. Previamente, trabaj\u00e9 como analista comercial de grandes cuentas en S4 Agtech, una empresa con sede en Buenos Aires especializada en seguros param\u00e9tricos de sequ\u00eda para agricultura. En esa posici\u00f3n, combin\u00e9 responsabilidades comerciales y anal\u00edticas, ayudando a los clientes a entender el riesgo de su cartera a trav\u00e9s de datos satelitales e \u00edndices de vegetaci\u00f3n, y explicando c\u00f3mo la cobertura de seguros param\u00e9tricos pod\u00eda minimizar esos riesgos. Trabajamos directamente con reaseguradoras como Munich Re en Argentina, Uruguay y Brasil, traduciendo an\u00e1lisis geoespaciales complejos en estrategias de gesti\u00f3n de riesgo accionables.</p> <p>Con una s\u00f3lida formaci\u00f3n en GIS, programaci\u00f3n en Python, Google Earth Engine y desarrollo de bases de datos Postgres/PostGIS, dise\u00f1o e implemento estrategias de innovaci\u00f3n que integran ciencia de datos geoespaciales, teledetecci\u00f3n, automatizaci\u00f3n de procesos y optimizaci\u00f3n impulsada por IA. Mi enfoque siempre est\u00e1 en la eficiencia, escalabilidad y asegurar que las soluciones sean verdaderamente adoptadas por los equipos que las utilizan.</p> <p>Soy Ingeniero Agr\u00f3nomo de la Universidad de Buenos Aires con una Maestr\u00eda en GIS Open Source y estudios de posgrado en Agronegocios, Ciencia de Datos y Blockchain.</p> <p>M\u00e1s all\u00e1 de la agricultura y la tecnolog\u00eda, he explorado las aplicaciones de blockchain y criptomonedas en fintech y ag-tech, y mantengo un fuerte lado creativo a trav\u00e9s de la m\u00fasica. Estudi\u00e9 trompeta de jazz durante varios a\u00f1os y me present\u00e9 como solista en diferentes big bands profesionales y combos peque\u00f1os de jazz. Tambi\u00e9n toco el piano desde los seis a\u00f1os, enfoc\u00e1ndome en repertorio cl\u00e1sico, rom\u00e1ntico e impresionista \u2014 un viaje art\u00edstico que ha moldeado profundamente mi disciplina, sensibilidad y creatividad.</p>"},{"location":"#por-que-trabajar-conmigo","title":"\u00bfPor qu\u00e9 trabajar conmigo?","text":"<p>Esto es lo que me diferencia y c\u00f3mo puedo ayudar a generar valor para tu negocio:</p> <ul> <li> <p> Experiencia empresarial comprobada</p> <p>Vengo de una familia de productores agropecuarios, tengo formaci\u00f3n agron\u00f3mica y una carrera construida alrededor de operaciones agr\u00edcolas. Traduzco las necesidades del negocio en soluciones claras y accionables, haciendo que el puente entre \"lo que el campo necesita\" y \"lo que construimos\" sea r\u00e1pido, natural y sin fricciones.</p> </li> <li> <p> Educador y comunicador</p> <p>Mi experiencia como creador de contenido y educador significa que puedo desglosar conceptos t\u00e9cnicos complejos en insights claros y accionables. Siempre vas a entender el 'por qu\u00e9' detr\u00e1s de las decisiones t\u00e9cnicas y vas a recibir actualizaciones claras del progreso.</p> </li> <li> <p> Experto de la industria</p> <p>Con m\u00e1s de 15 a\u00f1os en datos geoespaciales y teledetecci\u00f3n, ayudo a los equipos a convertir realidades espaciales y agron\u00f3micas complejas en sistemas confiables y accionables \u2014 bas\u00e1ndome en experiencia pr\u00e1ctica en agricultura, seguros, ONGs de investigaci\u00f3n, valuaci\u00f3n/divisi\u00f3n de tierras y operaciones a gran escala.</p> </li> <li> <p> Implementaci\u00f3n r\u00e1pida</p> <p>Me especializo en desarrollo e implementaci\u00f3n r\u00e1pida de soluciones de IA. Usando herramientas modernas y frameworks probados, puedo ayudarte a pasar del concepto a la producci\u00f3n m\u00e1s r\u00e1pido, d\u00e1ndote una ventaja competitiva en el mercado acelerado de hoy.</p> </li> </ul>"},{"location":"#lo-que-dicen-mis-clientes-anteriores-sobre-mi-trabajo","title":"Lo que dicen mis clientes anteriores sobre mi trabajo","text":"<ul> <li> <p> Gabriel V\u00e1zquez Am\u00e1bile</p> <p>Ingeniero Agr\u00f3nomo (UBA), Ph.D. y Master of Science (Purdue University), Administrador de Campos y Productor, Consultor Ambiental en IPCC, PUMA, F.T di Tella, Banco Mundial, FAO, AACREA e INTA.</p> <p>\"Tengo el placer de recomendar a Joaqu\u00edn Urruti como un profesional altamente calificado. Como Ingeniero Agr\u00f3nomo, lidera consistentemente en la adopci\u00f3n de herramientas avanzadas, tecnolog\u00edas y enfoques anal\u00edticos para abordar la variabilidad espacial y temporal en sistemas agr\u00edcolas, entregando insights valiosos basados en datos para los tomadores de decisiones.</p> <p>M\u00e1s all\u00e1 de su experiencia t\u00e9cnica, Joaqu\u00edn es un profesional confiable y comprometido que trabaja excepcionalmente bien en equipos. Su enfoque proactivo, sentido de responsabilidad y capacidad para generar sinergia lo convierten en un contribuidor valioso para cualquier entorno colaborativo.</p> <p>Recomiendo encarecidamente a Joaqu\u00edn a cualquier organizaci\u00f3n o proyecto que busque experiencia de alto nivel en an\u00e1lisis geoespacial aplicado a la producci\u00f3n agr\u00edcola y evaluaci\u00f3n del uso del suelo.\"</p> </li> <li> <p> Felipe Harrison</p> <p>Co-Fundador en AgAnalyst Ltd.</p> <p>\"He tenido el placer de trabajar con Joaqu\u00edn durante los \u00faltimos dos a\u00f1os, y destacar\u00eda especialmente su alto nivel de profesionalismo y enfoque riguroso para entregar soluciones tecnol\u00f3gicas para operaciones. Siempre est\u00e1 atento a las nuevas tecnolog\u00edas y c\u00f3mo aplicarlas a casos de uso del mundo real.</p> <p>Joaqu\u00edn se destaca por su capacidad para dise\u00f1ar e implementar sistemas robustos y escalables alineados con las necesidades del negocio. Es una persona confiable y colaborativa, lo que hace que trabajar con \u00e9l sea consistentemente una experiencia muy positiva. Lo recomiendo sin dudarlo.\"</p> </li> <li> <p> Galbusera Sebastian</p> <p>Co-Fundador y Chief Product Officer en Plataforma PUMA</p> <p>\"Joaqu\u00edn demuestra excelente aptitud para desarrollar soluciones innovadoras, es proactivo en la investigaci\u00f3n y procesamiento de informaci\u00f3n, dise\u00f1a herramientas de comunicaci\u00f3n t\u00e9cnica efectivas y trabaja excepcionalmente bien en entornos colaborativos.\"</p> </li> <li> <p> Francisco Lomazzi</p> <p>Gerente de Sustentabilidad en Espartina S.A.</p> <p>\"Joaqu\u00edn es alguien que est\u00e1 constantemente buscando mejorar las cosas que hace \u2014 no se conforma con que simplemente funcionen. Quiero destacar especialmente su creatividad, esfuerzo y dedicaci\u00f3n, que lo impulsan a buscar la excelencia continuamente.</p> <p>Hemos trabajado juntos en varios proyectos relacionados con sustentabilidad, pero me gustar\u00eda destacar uno en particular: cre\u00f3 una automatizaci\u00f3n para evaluar el cumplimiento legal de los campos respecto a los requisitos de distancia de aplicaci\u00f3n de agroqu\u00edmicos. Fue mucho trabajo, pero lo entreg\u00f3 exitosamente.</p> <p>\u00a1Un verdadero placer trabajar con Joaqu\u00edn!\"</p> </li> <li> <p> Javier Moreira de Souza</p> <p>Especialista en Agricultura Digital | Consultor Agr\u00edcola.</p> <p>\"Una persona intelectualmente aguda, mucha adrenalina al trabajar juntos, muy profesional y agradable, fue un placer haber trabajado juntos.\"</p> </li> </ul>"},{"location":"#preguntas-frecuentes","title":"Preguntas frecuentes","text":"\u00bfQu\u00e9 tan r\u00e1pido pod\u00e9s empezar a trabajar en mi proyecto? <p>T\u00edpicamente puedo comenzar nuevos proyectos dentro de 1-2 semanas de la firma del contrato. Para asuntos urgentes, mantengo cierta flexibilidad para situaciones de respuesta r\u00e1pida y potencialmente puedo comenzar antes \u2014 solo contame tu timeline durante nuestra consulta inicial.</p> \u00bfRequer\u00eds un tama\u00f1o m\u00ednimo de proyecto o compromiso? <p>Aunque puedo acomodar proyectos de cualquier tama\u00f1o, encuentro que los compromisos de al menos 20 horas permiten un impacto significativo. Esto nos da tiempo suficiente para entender tus datos, implementar soluciones y entregar resultados accionables. Podemos empezar con un peque\u00f1o proyecto piloto para asegurarnos de que somos un buen fit.</p> \u00bfEn qu\u00e9 industrias ten\u00e9s experiencia? <p>He entregado proyectos exitosamente en operaciones agr\u00edcolas, administraciones, valuaciones de campos y servicios de seguros param\u00e9tricos. Aunque me especializo en IA, geoespacial y teledetecci\u00f3n, tambi\u00e9n aplico mi conocimiento de IA y Python en proyectos que involucran todas las \u00e1reas de la empresa.</p> \u00bfC\u00f3mo manej\u00e1s la seguridad y confidencialidad de los datos? <p>Me tomo la seguridad de datos extremadamente en serio. Firmo NDAs comprensivos antes de comenzar cualquier proyecto, uso encriptaci\u00f3n de nivel empresarial para todas las transferencias de datos y sigo las mejores pr\u00e1cticas de la industria para el manejo de datos. Tambi\u00e9n puedo trabajar dentro de tu infraestructura y pol\u00edticas de seguridad existentes.</p> \u00bfCu\u00e1l es tu estructura de precios? <p>Ofrezco modelos de precios tanto por proyecto como por retainer. Las tarifas de proyecto se basan en alcance, complejidad y valor entregado en lugar de horas trabajadas. Para soporte continuo, ofrezco paquetes de retainer flexibles, y tambi\u00e9n puedo proporcionar soporte continuo por hora cuando eso se ajusta mejor a tus necesidades. Discutamos tus requerimientos espec\u00edficos durante nuestra consulta para determinar el enfoque m\u00e1s rentable.</p> \u00bfC\u00f3mo comunic\u00e1s el progreso y los resultados? <p>Mantengo una comunicaci\u00f3n clara a trav\u00e9s de actualizaciones semanales de progreso y reuniones regulares de seguimiento. Vas a recibir documentaci\u00f3n detallada de todos los an\u00e1lisis, hallazgos y recomendaciones.</p> <ul> <li> <p> \u00a1Tomemos un caf\u00e9 virtual juntos!</p> <p>\u00bfQuer\u00e9s ver si somos un buen match? Charlemos y descubr\u00e1moslo. Agend\u00e1 una sesi\u00f3n de estrategia gratuita de 30 minutos para discutir tus desaf\u00edos y explorar c\u00f3mo podemos trabajar juntos.</p> <p>Agend\u00e1 una llamada gratuita </p> </li> </ul>"},{"location":"blog/","title":"Blog","text":""},{"location":"blog/2026/02/15/procesamiento-de-datos-de-precipitaci%C3%B3n-pmra-de-rasters-satelitales-a-insights-agr%C3%ADcolas/","title":"Procesamiento de Datos de Precipitaci\u00f3n PMRA: De Rasters Satelitales a Insights Agr\u00edcolas","text":"<p>Al evaluar campos para arrendamiento en Argentina, siempre surge una pregunta: \u00bfcu\u00e1nta lluvia recibe realmente esta zona? El problema es que las estaciones meteorol\u00f3gicas terrestres son escasas y no cubren la mayor\u00eda de las \u00e1reas rurales. Los datos satelitales como ERA5 tienen baja resoluci\u00f3n espacial y precisi\u00f3n cuestionable para decisiones locales.</p> <p>La buena noticia es que el dataset PMRA (Precipitaciones Mensuales de la Rep\u00fablica Argentina) de CONICET resuelve esto. Combina mediciones terrestres con cuatro productos globales de precipitaci\u00f3n mediante regresi\u00f3n de random forest. Esto nos da datos de precipitaci\u00f3n con resoluci\u00f3n de 5km para cada rinc\u00f3n de Argentina desde 2000 hasta 2023.</p> <p>Pero hay un desaf\u00edo: los datos vienen como 288 archivos raster individuales. Un archivo por mes durante 24 a\u00f1os. Trabajar con ellos uno por uno es impracticable cuando necesit\u00e1s analizar tendencias, calcular estad\u00edsticas o comparar regiones.</p>"},{"location":"blog/2026/02/15/procesamiento-de-datos-de-precipitaci%C3%B3n-pmra-de-rasters-satelitales-a-insights-agr%C3%ADcolas/#por-que-esto-importa-para-las-operaciones-agricolas","title":"Por Qu\u00e9 Esto Importa para las Operaciones Agr\u00edcolas","text":"<p>En mi rol liderando innovaci\u00f3n en una empresa agr\u00edcola, evaluamos constantemente nuevas tierras para arrendamiento. Entender los patrones de precipitaci\u00f3n es cr\u00edtico para:</p> <ul> <li>Estimar el potencial de rendimiento de cultivos en regiones desconocidas</li> <li>Evaluar el riesgo de sequ\u00eda para decisiones de seguros</li> <li>Planificar estrategias de rotaci\u00f3n basadas en la distribuci\u00f3n hist\u00f3rica de lluvias</li> </ul> <p>Sin datos completos de precipitaci\u00f3n, estamos tomando decisiones costosas con informaci\u00f3n incompleta. PMRA llena ese vac\u00edo, pero solo si podemos procesarlo eficientemente.</p>"},{"location":"blog/2026/02/15/procesamiento-de-datos-de-precipitaci%C3%B3n-pmra-de-rasters-satelitales-a-insights-agr%C3%ADcolas/#el-dataset-pmra-con-que-estas-trabajando","title":"El Dataset PMRA: Con Qu\u00e9 Est\u00e1s Trabajando","text":"<p>El dataset PMRA proporciona datos mensuales de precipitaci\u00f3n con resoluci\u00f3n de 5km en toda Argentina. Cada archivo GeoTIFF cubre el pa\u00eds entero para un mes. La convenci\u00f3n de nombres sigue este patr\u00f3n: <code>PMRA_[mes]_[a\u00f1o].tif</code>.</p> <p>Descarg\u00e1 el dataset completo desde la carpeta de Drive de CONICET. Obtendr\u00e1s 288 archivos que totalizan varios gigabytes con datos de precipitaci\u00f3n. </p> <p>Adem\u00e1s, desarrollaron una app en Google Earth Engine que permite visualizar los 23 mapas anuales de precipitaci\u00f3n,  delimitar un \u00e1rea de inter\u00e9s y generar un gr\u00e1fico de la serie temporal de precipitaciones mensuales</p> <p></p> <p>La ventaja clave sobre ERA5 u otros productos satelitales es la combinaci\u00f3n de datos de campo con datos satelitales. Este enfoque h\u00edbrido entrega mejor precisi\u00f3n local sin requerir una estaci\u00f3n meteorol\u00f3gica en tu \u00e1rea espec\u00edfica.</p>"},{"location":"blog/2026/02/15/procesamiento-de-datos-de-precipitaci%C3%B3n-pmra-de-rasters-satelitales-a-insights-agr%C3%ADcolas/#por-que-xarray-cambia-todo","title":"Por Qu\u00e9 XArray Cambia Todo","text":"<p>La mayor\u00eda de los profesionales GIS est\u00e1n familiarizados con trabajar con rasters individuales usando herramientas como <code>rasterio</code> o GDAL. El flujo de trabajo est\u00e1ndar se ve as\u00ed:</p> <ol> <li>Abrir archivo raster</li> <li>Extraer datos</li> <li>Procesar</li> <li>Cerrar archivo</li> <li>Repetir 287 veces m\u00e1s</li> </ol> <p>Este enfoque se rompe cuando necesit\u00e1s analizar patrones temporales. XArray trata tus 288 rasters como un \u00fanico dataset multidimensional organizado por tiempo, latitud y longitud.</p> <p></p> <p>En lugar de gestionar 288 archivos, trabaj\u00e1s con una sola estructura de datos unificada. Esto hace que las consultas temporales sean instant\u00e1neas: \"\u00bfCu\u00e1l fue la precipitaci\u00f3n promedio en esta ubicaci\u00f3n en todos los eneros?\" se convierte en una sola l\u00ednea de c\u00f3digo.</p>"},{"location":"blog/2026/02/15/procesamiento-de-datos-de-precipitaci%C3%B3n-pmra-de-rasters-satelitales-a-insights-agr%C3%ADcolas/#configuracion-del-entorno","title":"Configuraci\u00f3n del Entorno","text":"<p>Primero, instal\u00e1 las librer\u00edas necesarias:</p> <pre><code>pip install rioxarray xarray numpy pandas geopandas matplotlib python-dotenv\n</code></pre> <p>Las importaciones principales para este flujo de trabajo:</p> <pre><code>import rioxarray\nimport numpy as np\nimport xarray as xr\nimport matplotlib.pyplot as plt\nimport os, glob\nfrom pathlib import Path\nimport pandas as pd\nimport geopandas as gpd\nimport rasterio\nimport datetime\nfrom dotenv import load_dotenv\n\nload_dotenv()\nxr.set_options(keep_attrs=True, display_expand_data=False)\n</code></pre> <p>Uso <code>python-dotenv</code> para mantener las rutas de archivos en variables de entorno en lugar de hardcodearlas. Esto hace que el c\u00f3digo sea portable entre diferentes m\u00e1quinas y mantiene las rutas sensibles fuera del control de versiones.</p>"},{"location":"blog/2026/02/15/procesamiento-de-datos-de-precipitaci%C3%B3n-pmra-de-rasters-satelitales-a-insights-agr%C3%ADcolas/#cargando-y-organizando-288-rasters","title":"Cargando y Organizando 288 Rasters","text":"<p>El primer desaf\u00edo es leer todos los archivos raster y organizarlos cronol\u00f3gicamente. Los nombres de archivo contienen el mes y el a\u00f1o, pero necesitamos parsearlos a objetos datetime correctos.</p> <pre><code>rasters_path = os.getenv('RASTERS_PATH')\n\nraster_files = str(Path(rasters_path) / '*.tif')\nfile_names = [os.path.basename(x) for x in glob.glob(raster_files)]\nfile_paths = [os.path.abspath(x) for x in glob.glob(raster_files)]\n\nmonths_str_list = ['ene', 'feb', 'mar', 'abr', 'may', 'jun',\n                   'jul', 'ago', 'sep', 'oct', 'nov', 'dic']\nmonths_num_list = range(1, 13)\nmy_dict = dict(zip(months_str_list, months_num_list))\n\ndates = []\nfor file in file_names:\n    file_name = Path(file).name\n    month_str = file_name[5:8]\n    year = file_name[-8:-4]\n    for key, value in my_dict.items():\n        if month_str == key:\n            month = value\n    date = pd.to_datetime(f'{year}-{month}')\n    dates.append(date)\n</code></pre> <p>Esto extrae las abreviaturas de meses en espa\u00f1ol y los a\u00f1os de los nombres de archivo, luego los convierte a objetos datetime de Python. Estos se convierten en la dimensi\u00f3n temporal de nuestro dataset XArray.</p>"},{"location":"blog/2026/02/15/procesamiento-de-datos-de-precipitaci%C3%B3n-pmra-de-rasters-satelitales-a-insights-agr%C3%ADcolas/#construyendo-el-dataset-xarray","title":"Construyendo el Dataset XArray","text":"<p>Aqu\u00ed es donde XArray muestra su poder. En lugar de iterar sobre archivos, concatenamos todos los rasters a lo largo de la dimensi\u00f3n temporal en una sola operaci\u00f3n:</p> <pre><code>time_var = xr.Variable('time', dates)\n\ngeotiffs_da = xr.concat([rioxarray.open_rasterio(i) for i in file_paths],\n                        dim=time_var)\n\ngeotiffs_ds = geotiffs_da.to_dataset(\"band\")\ngeotiffs_ds = geotiffs_ds.rename({1: 'precipitacion'})\n</code></pre> <p>Lo que obtenemos es un Dataset con dimensiones <code>(time: 288, y: 851, x: 713)</code>. Eso son 288 meses, 851 puntos de latitud y 713 puntos de longitud. Cada p\u00edxel ahora tiene un historial completo de precipitaciones de 24 a\u00f1os.</p> <p>La estructura de salida se ve as\u00ed:</p> <pre><code>&lt;xarray.Dataset&gt;\nDimensions:        (time: 288, y: 851, x: 713)\nCoordinates:\n  * x              (x) float64 -73.86 -73.82 -73.77 ... -52.75 -52.71 -52.66\n  * y              (y) float64 -55.4 -55.36 -55.31 ... -21.22 -21.18 -21.13\n    spatial_ref    int64 0\n  * time           (time) datetime64[ns] 2001-05-01 2015-05-01 ... 2018-05-01\nData variables:\n    precipitacion  (time, y, x) float32 nan nan nan nan nan ... nan nan nan nan\n</code></pre>"},{"location":"blog/2026/02/15/procesamiento-de-datos-de-precipitaci%C3%B3n-pmra-de-rasters-satelitales-a-insights-agr%C3%ADcolas/#exportando-a-netcdf-para-reutilizacion","title":"Exportando a NetCDF para Reutilizaci\u00f3n","text":"<p>Una vez que construiste el dataset XArray, guardalo como NetCDF. Este es un formato binario autodescriptivo dise\u00f1ado para datos cient\u00edficos:</p> <pre><code>out_file = Path(raster_files).parent.parent / 'precipitaciones_arg_PMRA.nc'\ngeotiffs_ds.to_netcdf(out_file)\n</code></pre> <p>Los archivos NetCDF preservan todos los metadatos y se cargan mucho m\u00e1s r\u00e1pido que procesar 288 rasters individuales. La pr\u00f3xima vez que necesites estos datos, pod\u00e9s cargar el dataset completo de 24 a\u00f1os en segundos en lugar de minutos.</p>"},{"location":"blog/2026/02/15/procesamiento-de-datos-de-precipitaci%C3%B3n-pmra-de-rasters-satelitales-a-insights-agr%C3%ADcolas/#convirtiendo-a-dataframe-para-analisis","title":"Convirtiendo a DataFrame para An\u00e1lisis","text":"<p>XArray es genial para operaciones espaciales, pero la mayor\u00eda de las herramientas de an\u00e1lisis esperan datos tabulares. Convertir a un DataFrame de Pandas te da acceso al ecosistema completo de librer\u00edas de ciencia de datos de Python:</p> <pre><code>precipitacion_df = geotiffs_ds.to_dataframe().reset_index()\nprecipitacion_df = precipitacion_df[['time', 'x', 'y', 'precipitacion']]\nprecipitacion_df = precipitacion_df.dropna()\nprecipitacion_df.precipitacion = precipitacion_df.precipitacion.astype('int8')\n</code></pre> <p>Esto crea un DataFrame de m\u00e1s de 40 millones de filas con columnas para tiempo, coordenadas y valores de precipitaci\u00f3n. Cada fila representa un p\u00edxel en un punto en el tiempo.</p> <p>El paso <code>.dropna()</code> es importante porque muchos p\u00edxeles caen fuera de las fronteras de Argentina y contienen valores nulos. Eliminarlos reduce el tama\u00f1o del archivo y acelera el an\u00e1lisis.</p>"},{"location":"blog/2026/02/15/procesamiento-de-datos-de-precipitaci%C3%B3n-pmra-de-rasters-satelitales-a-insights-agr%C3%ADcolas/#creando-un-geodataframe-para-analisis-espacial","title":"Creando un GeoDataFrame para An\u00e1lisis Espacial","text":"<p>Si necesit\u00e1s realizar operaciones espaciales como overlays o spatial joins, convert\u00ed a GeoDataFrame:</p> <pre><code>from shapely.geometry import Point\n\nprecipitacion_df['geometry'] = [Point(xy) for xy in zip(precipitacion_df['x'],\n                                                         precipitacion_df['y'])]\ngdf = gpd.GeoDataFrame(precipitacion_df)\n</code></pre> <p>Esto agrega una columna de geometr\u00eda con objetos Point para cada par de coordenadas. Ahora pod\u00e9s:</p> <ul> <li>Hacer spatial join con l\u00edmites de campos</li> <li>Filtrar por regi\u00f3n usando pol\u00edgonos</li> <li>Calcular estad\u00edsticas zonales para \u00e1reas espec\u00edficas</li> <li>Exportar a formatos GIS como GeoPackage o Shapefile</li> </ul> <pre><code>out_file = Path(raster_files).parent.parent / 'precipitaciones_arg_PMRA.gpkg'\ngdf.to_file(out_file, driver='GPKG')\n</code></pre>"},{"location":"blog/2026/02/15/procesamiento-de-datos-de-precipitaci%C3%B3n-pmra-de-rasters-satelitales-a-insights-agr%C3%ADcolas/#aplicacion-real-evaluacion-de-campos","title":"Aplicaci\u00f3n Real: Evaluaci\u00f3n de Campos","text":"<p>Cuando evaluamos un campo potencial para arrendamiento, ejecuto este flujo de trabajo para extraer datos de precipitaci\u00f3n para esa ubicaci\u00f3n espec\u00edfica. El proceso toma minutos:</p> <ol> <li>Cargar el l\u00edmite del campo como un pol\u00edgono</li> <li>Filtrar espacialmente el GeoDataFrame a puntos dentro del l\u00edmite</li> <li>Agrupar por tiempo y calcular la precipitaci\u00f3n media</li> <li>Generar estad\u00edsticas mensuales y anuales</li> <li>Comparar contra benchmarks regionales</li> </ol> <p>Esto nos da un historial de precipitaciones de 24 a\u00f1os para un campo que nunca cultivamos. Podemos identificar a\u00f1os propensos a sequ\u00eda, calcular la lluvia de la temporada de crecimiento y evaluar la variabilidad.</p> <p>Sin los datos de PMRA, estar\u00edamos dependiendo de la estaci\u00f3n meteorol\u00f3gica m\u00e1s cercana, que podr\u00eda estar a 50km en un microclima diferente. Con PMRA, tenemos datos espec\u00edficos del sitio con resoluci\u00f3n de 5km.</p>"},{"location":"blog/2026/02/15/procesamiento-de-datos-de-precipitaci%C3%B3n-pmra-de-rasters-satelitales-a-insights-agr%C3%ADcolas/#conclusiones-clave","title":"Conclusiones Clave","text":"<p>No necesit\u00e1s estaciones meteorol\u00f3gicas para analizar precipitaciones en cualquier lugar de Argentina. PMRA proporciona datos validados de alta resoluci\u00f3n que combinan la precisi\u00f3n de mediciones terrestres con la cobertura de datos satelitales.</p> <p>XArray es la herramienta correcta para este trabajo porque maneja datos geoespaciales multidimensionales de forma nativa. Intentar procesar 288 rasters individualmente es ineficiente y propenso a errores.</p> <p>Los formatos de salida importan. NetCDF para recarga r\u00e1pida, DataFrames para an\u00e1lisis estad\u00edstico, GeoDataFrames para operaciones espaciales. Eleg\u00ed el formato que se ajuste a tu flujo de trabajo posterior.</p>"},{"location":"blog/2026/02/15/procesamiento-de-datos-de-precipitaci%C3%B3n-pmra-de-rasters-satelitales-a-insights-agr%C3%ADcolas/#proximos-pasos","title":"Pr\u00f3ximos Pasos","text":"<p>Si est\u00e1s trabajando con datos agr\u00edcolas en Argentina y necesit\u00e1s incorporar an\u00e1lisis de precipitaci\u00f3n en tu toma de decisiones, este flujo de trabajo es un punto de partida. El mismo enfoque funciona para otros datasets de series temporales grillados como temperatura, NDVI o humedad del suelo.</p> <p>Siempre me interesa discutir c\u00f3mo el an\u00e1lisis de datos espaciales puede mejorar las operaciones agr\u00edcolas. Si est\u00e1s enfrentando desaf\u00edos similares o quer\u00e9s explorar c\u00f3mo estas t\u00e9cnicas se aplican a tu caso de uso espec\u00edfico, conectemos.</p> <p>Agendar Llamada Introductoria Gratuita </p> <p>Tambi\u00e9n pod\u00e9s encontrarme en LinkedIn donde comparto m\u00e1s sobre GIS, Python e innovaci\u00f3n agr\u00edcola.</p>"},{"location":"blog/2026/01/03/ejecutando-procesos-largos-sin-interrupciones-con-el-comando-caffeinate-de-macos/","title":"Ejecutando Procesos Largos sin Interrupciones con el comando \"caffeinate\" de MacOS","text":""},{"location":"blog/2026/01/03/ejecutando-procesos-largos-sin-interrupciones-con-el-comando-caffeinate-de-macos/#por-que-usar-el-comando-caffeinate","title":"\u00bfPor qu\u00e9 usar el comando \"caffeinate\"?","text":"<p>El modo de suspensi\u00f3n de MacOS interrumpe procesos de larga duraci\u00f3n como an\u00e1lisis de datos GIS, procesamiento de im\u00e1genes satelitales o migraciones de bases de datos. Un trabajo de procesamiento raster de 6 horas puede ser terminado cuando el sistema entra en modo de suspensi\u00f3n, desperdiciando horas de computaci\u00f3n.</p>"},{"location":"blog/2026/01/03/ejecutando-procesos-largos-sin-interrupciones-con-el-comando-caffeinate-de-macos/#solucion-uso-estrategico-de-caffeinate","title":"Soluci\u00f3n: Uso Estrat\u00e9gico de Caffeinate","text":"<p>El comando incorporado <code>caffeinate</code> previene la suspensi\u00f3n del sistema solo cuando es necesario, manteniendo la eficiencia energ\u00e9tica mientras asegura que los procesos cr\u00edticos se completen sin interrupci\u00f3n.</p>"},{"location":"blog/2026/01/03/ejecutando-procesos-largos-sin-interrupciones-con-el-comando-caffeinate-de-macos/#como-funciona-caffeinate","title":"C\u00f3mo Funciona Caffeinate","text":"<p><code>caffeinate</code> crea aserciones que modifican el comportamiento de suspensi\u00f3n del sistema. Opera en dos modos:</p> <p>Modo Wrapper: Cuando se especifica un programa, <code>caffeinate</code> crea aserciones para esa utilidad. Las aserciones permanecen activas durante la ejecuci\u00f3n y se liberan autom\u00e1ticamente al completarse.</p> <p>Modo Directo: Sin una utilidad especificada, <code>caffeinate</code> crea aserciones directamente y las mantiene activas hasta que se termina manualmente (<code>Ctrl+C</code>) o se cierra la terminal.</p>"},{"location":"blog/2026/01/03/ejecutando-procesos-largos-sin-interrupciones-con-el-comando-caffeinate-de-macos/#opciones-disponibles","title":"Opciones Disponibles","text":""},{"location":"blog/2026/01/03/ejecutando-procesos-largos-sin-interrupciones-con-el-comando-caffeinate-de-macos/#-d-display-previene-que-la-pantalla-entre-en-suspension","title":"<code>-d</code> (display): Previene que la pantalla entre en suspensi\u00f3n.","text":"<p>Caso de uso: Monitoreo visual de procesos, presentaciones, dashboards en tiempo real.</p>"},{"location":"blog/2026/01/03/ejecutando-procesos-largos-sin-interrupciones-con-el-comando-caffeinate-de-macos/#-i-idle","title":"<code>-i</code> (idle)","text":"<p>Previene que el sistema entre en suspensi\u00f3n por inactividad.</p> <p>Caso de uso: Procesos batch, scripts de an\u00e1lisis, operaciones en segundo plano sin interacci\u00f3n del usuario.</p>"},{"location":"blog/2026/01/03/ejecutando-procesos-largos-sin-interrupciones-con-el-comando-caffeinate-de-macos/#-m-mediadisk","title":"<code>-m</code> (media/disk)","text":"<p>Previene que el disco entre en suspensi\u00f3n por inactividad.</p> <p>Caso de uso: Operaciones intensivas de I/O, migraciones de bases de datos, backups, escritura continua de logs.</p>"},{"location":"blog/2026/01/03/ejecutando-procesos-largos-sin-interrupciones-con-el-comando-caffeinate-de-macos/#-s-system-on-ac","title":"<code>-s</code> (system on AC)","text":"<p>Previene que todo el sistema entre en suspensi\u00f3n.</p> <p>Importante: Solo v\u00e1lido cuando se ejecuta con alimentaci\u00f3n AC. Se ignora cuando est\u00e1 en bater\u00eda.</p> <p>Caso de uso: Servidores de desarrollo local, procesos cr\u00edticos que requieren ejecuci\u00f3n continua garantizada.</p>"},{"location":"blog/2026/01/03/ejecutando-procesos-largos-sin-interrupciones-con-el-comando-caffeinate-de-macos/#-u-user-activity","title":"<code>-u</code> (user activity)","text":"<p>Declara que el usuario est\u00e1 activo.</p> <p>Comportamiento especial:</p> <ul> <li>Enciende autom\u00e1ticamente la pantalla si est\u00e1 apagada</li> <li>Previene que la pantalla entre en suspensi\u00f3n por inactividad</li> <li>Usa un timeout predeterminado de 5 segundos si no se especifica <code>-t</code></li> </ul> <p>Caso de uso: Scripts que requieren simular actividad de usuario, mantener sesiones SSH activas.</p>"},{"location":"blog/2026/01/03/ejecutando-procesos-largos-sin-interrupciones-con-el-comando-caffeinate-de-macos/#-t-segundos-timeout","title":"<code>-t</code> <code>&lt;segundos&gt;</code> (timeout)","text":"<p>Especifica el tiempo de validez de la aserci\u00f3n en segundos.</p> <p>Importante: No se usa en modo wrapper, ya que la duraci\u00f3n se controla por la ejecuci\u00f3n del programa.</p> <p>Caso de uso: Mantener el sistema activo por un per\u00edodo espec\u00edfico conocido (ej., 3600 segundos = 1 hora).</p>"},{"location":"blog/2026/01/03/ejecutando-procesos-largos-sin-interrupciones-con-el-comando-caffeinate-de-macos/#-w-pid-wait-for-process","title":"<code>-w</code> <code>&lt;pid&gt;</code> (wait for process)","text":"<p>Espera a que el proceso con el PID especificado termine. La aserci\u00f3n se libera autom\u00e1ticamente cuando el proceso termina.</p> <p>Importante: Se ignora cuando se usa en modo wrapper.</p> <p>Caso de uso: Mantener el sistema activo mientras un proceso espec\u00edfico en ejecuci\u00f3n contin\u00faa trabajando.</p>"},{"location":"blog/2026/01/03/ejecutando-procesos-largos-sin-interrupciones-con-el-comando-caffeinate-de-macos/#resumen-de-combinaciones-comunes-de-flags","title":"Resumen de Combinaciones Comunes de Flags","text":"Flags Descripci\u00f3n Caso de Uso <code>-i</code> Solo previene suspensi\u00f3n por inactividad Scripts Python, procesos batch <code>-di</code> Pantalla + suspensi\u00f3n por inactividad Monitoreo visual de procesos <code>-ims</code> Sistema + disco + inactividad (pantalla APAGADA) Procesamiento eficiente en segundo plano <code>-dims</code> Protecci\u00f3n completa (pantalla ENCENDIDA) Operaciones cr\u00edticas de DB/disco con monitoreo <code>-i -t 3600</code> Suspensi\u00f3n por inactividad por 1 hora Proceso con duraci\u00f3n conocida <code>-u</code> Simula actividad de usuario Mantener sesiones activas"},{"location":"blog/2026/01/03/ejecutando-procesos-largos-sin-interrupciones-con-el-comando-caffeinate-de-macos/#por-que-se-recomienda-ims","title":"Por qu\u00e9 se Recomienda <code>-ims</code>","text":"<p>La combinaci\u00f3n <code>-ims</code> es ideal para procesos de larga duraci\u00f3n en segundo plano porque:</p> <ul> <li>Eficiente energ\u00e9ticamente: Permite que la pantalla entre en suspensi\u00f3n, ahorrando energ\u00eda</li> <li>Protecci\u00f3n del sistema: Previene suspensi\u00f3n del sistema y disco por inactividad</li> <li>Conciencia de alimentaci\u00f3n AC: El flag <code>-s</code> asegura protecci\u00f3n solo cuando est\u00e1 enchufado</li> <li>Perfecto para trabajos nocturnos: Procesamiento GIS, operaciones de base de datos, pipelines de datos</li> </ul>"},{"location":"blog/2026/01/03/ejecutando-procesos-largos-sin-interrupciones-con-el-comando-caffeinate-de-macos/#ejemplos-de-implementacion","title":"Ejemplos de Implementaci\u00f3n","text":""},{"location":"blog/2026/01/03/ejecutando-procesos-largos-sin-interrupciones-con-el-comando-caffeinate-de-macos/#envolviendo-un-proceso-basico","title":"Envolviendo un Proceso B\u00e1sico","text":"<pre><code>caffeinate -i python analyze_ndvi_timeseries.py\n</code></pre>"},{"location":"blog/2026/01/03/ejecutando-procesos-largos-sin-interrupciones-con-el-comando-caffeinate-de-macos/#ejecucion-basada-en-tiempo","title":"Ejecuci\u00f3n Basada en Tiempo","text":"<pre><code># Mantener sistema activo por 8 horas\ncaffeinate -i -t 28800\n</code></pre>"},{"location":"blog/2026/01/03/ejecutando-procesos-largos-sin-interrupciones-con-el-comando-caffeinate-de-macos/#procesamiento-eficiente-en-segundo-plano","title":"Procesamiento Eficiente en Segundo Plano","text":"<pre><code>caffeinate -ims python batch_processing_overnight.py\n</code></pre>"},{"location":"blog/2026/01/03/ejecutando-procesos-largos-sin-interrupciones-con-el-comando-caffeinate-de-macos/#operaciones-criticas-con-monitoreo","title":"Operaciones Cr\u00edticas con Monitoreo","text":"<pre><code>caffeinate -dims python process_data.py\n</code></pre>"},{"location":"blog/2026/01/03/ejecutando-procesos-largos-sin-interrupciones-con-el-comando-caffeinate-de-macos/#operaciones-de-base-de-datos","title":"Operaciones de Base de Datos","text":"<pre><code>caffeinate -dims psql -f migration_script.sql\n</code></pre>"},{"location":"blog/2026/01/03/ejecutando-procesos-largos-sin-interrupciones-con-el-comando-caffeinate-de-macos/#monitorear-proceso-existente","title":"Monitorear Proceso Existente","text":"<pre><code>caffeinate -i -w 12345\n</code></pre>"},{"location":"blog/2026/01/03/ejecutando-procesos-largos-sin-interrupciones-con-el-comando-caffeinate-de-macos/#integracion-en-pipeline-de-automatizacion","title":"Integraci\u00f3n en Pipeline de Automatizaci\u00f3n","text":"<pre><code>#!/bin/bash\n# run_geo_pipeline.sh\n\ncaffeinate -ims python preprocess_sentinel2.py &amp;&amp; \\\npython train_crop_classifier.py &amp;&amp; \\\npython generate_yield_predictions.py\n</code></pre>"},{"location":"blog/2026/01/03/ejecutando-procesos-largos-sin-interrupciones-con-el-comando-caffeinate-de-macos/#resultados","title":"Resultados","text":"<ul> <li>Cero procesos interrumpidos: Completaci\u00f3n confiable de trabajos nocturnos</li> <li>Mejor utilizaci\u00f3n de recursos: Los sistemas entran en suspensi\u00f3n cuando est\u00e1n inactivos, ahorrando energ\u00eda</li> <li>Mejor debugging: La pantalla permanece activa para monitoreo cuando es necesario</li> <li>Despliegue simplificado: Un solo comando maneja la gesti\u00f3n de suspensi\u00f3n ```</li> </ul>"},{"location":"blog/2026/01/03/ejecutando-procesos-largos-sin-interrupciones-con-el-comando-caffeinate-de-macos/#trabajando-con-flujos-de-trabajo-de-datos-geoespaciales-similares","title":"\u00bfTrabajando con flujos de trabajo de datos geoespaciales similares?","text":"<p>Si est\u00e1s lidiando con pipelines de procesamiento interrumpidos o necesit\u00e1s ayuda optimizando tu infraestructura de automatizaci\u00f3n GIS, conectemos. Me especializo en construir sistemas de procesamiento de datos robustos para aplicaciones agr\u00edcolas y ambientales.</p> <p>Agend\u00e1 una llamada gratuita </p>"},{"location":"blog/2026/02/11/eliminando-la-carga-manual-de-datos-desde-pdfs-con-ia-un-pipeline-productivo/","title":"Eliminando la Carga Manual de Datos desde PDFs con IA: Un Pipeline Productivo","text":"<p>Un problema muy com\u00fan: las organizaciones suelen tener carpetas llenas de CVs en PDF y alguien tiene que extraer manualmente la informaci\u00f3n a una planilla \u2014 lento, propenso a errores e imposible de escalar. Constru\u00ed un pipeline para eliminar esa tarea por completo.</p>"},{"location":"blog/2026/02/11/eliminando-la-carga-manual-de-datos-desde-pdfs-con-ia-un-pipeline-productivo/#el-costo-real-no-es-el-tiempo-por-documento","title":"El Costo Real No Es el Tiempo Por Documento","text":"<p>Cuando alguien me dice \"la carga de datos toma 15 minutos por CV\", el instinto es calcular el ahorro por hora. Pero ese es el marco equivocado. El costo real es lo que no sucede porque el equipo est\u00e1 atascado haciendo transcripci\u00f3n manual \u2014 an\u00e1lisis que no se ejecutan, patrones que no se detectan, decisiones que se demoran.</p> <p>El pipeline que constru\u00ed procesa un directorio de PDFs, extrae cada campo relevante de cada documento, valida la salida contra un esquema estricto y agrega los resultados a un libro de Excel estructurado. Todo se ejecuta de forma desatendida como un cron job o en un Workflow de Prefect.</p>"},{"location":"blog/2026/02/11/eliminando-la-carga-manual-de-datos-desde-pdfs-con-ia-un-pipeline-productivo/#de-problema-de-rrhh-a-arquitectura-tecnica","title":"De Problema de RRHH a Arquitectura T\u00e9cnica","text":"<p>El punto de partida fue un pedido real: el equipo de RRHH necesitaba una base de datos de candidatos buscable a partir de una carpeta de CVs. Los documentos no ten\u00edan un formato consistente \u2014 algunos estaban dise\u00f1ados con estilos pesados, otros eran exportaciones de texto plano. No hab\u00eda dos CVs iguales.</p> <p>El pipeline tiene cuatro etapas:</p> <ol> <li>Descubrimiento de archivos y cach\u00e9 \u2014 recorrer el directorio, saltear archivos ya procesados usando hashing SHA-256</li> <li>Conversi\u00f3n de PDF a Markdown \u2014 parsear cada documento con Docling</li> <li>Extracci\u00f3n estructurada \u2014 enviar el Markdown a OpenAI con un esquema Pydantic aplicado via Structured Outputs</li> <li>Persistencia \u2014 agregar los datos extra\u00eddos a un libro Excel multi-hoja</li> </ol> <p></p>"},{"location":"blog/2026/02/11/eliminando-la-carga-manual-de-datos-desde-pdfs-con-ia-un-pipeline-productivo/#por-que-markdown-como-formato-intermedio","title":"Por Qu\u00e9 Markdown Como Formato Intermedio","text":"<p>Enviar bytes crudos de PDF a un LLM no es buena idea en la pr\u00e1ctica. Los PDFs son archivos binarios \u2014 la extracci\u00f3n de texto depende mucho de c\u00f3mo se gener\u00f3 el archivo, y la salida suele estar contaminada con artefactos de layout, problemas de codificaci\u00f3n de caracteres y orden de lectura desordenado.</p> <p>Docling convierte PDFs a Markdown limpio, preservando la estructura l\u00f3gica: encabezados, vi\u00f1etas, tablas. Esto importa porque el LLM lee el documento m\u00e1s como lo har\u00eda un humano, en lugar de intentar parsear un flujo de coordenadas y c\u00f3digos de glifos. Tambi\u00e9n reduce significativamente el conteo de tokens comparado con enfoques de extracci\u00f3n ingenuos.</p> <pre><code>from docling.document_converter import DocumentConverter\n\nconverter = DocumentConverter()\nresult = converter.convert(file_path)\nmarkdown_text = result.document.export_to_markdown()\n</code></pre> <p>La salida en Markdown es limpia, legible y \u00f3ptima para procesamiento por LLM. Si algo sale mal en la extracci\u00f3n, pod\u00e9s inspeccionar el archivo intermedio directamente.</p>"},{"location":"blog/2026/02/11/eliminando-la-carga-manual-de-datos-desde-pdfs-con-ia-un-pipeline-productivo/#la-pieza-que-lo-hace-productivo-pydantic-structured-outputs","title":"La Pieza Que Lo Hace Productivo: Pydantic + Structured Outputs","text":"<p>Este es el insight central. Una llamada est\u00e1ndar de chat completion devolver\u00e1 texto que parece JSON \u2014 hasta que no lo es. Los nombres de campos se desv\u00edan, los campos opcionales se omiten silenciosamente, las estructuras anidadas se aplanan inesperadamente. Ejecutar eso contra cientos de documentos significa que tu base de datos se llena lentamente de inconsistencias que son costosas de arreglar despu\u00e9s.</p> <p>OpenAI Structured Outputs cambia el contrato. En lugar de pedirle al modelo que \"por favor responda en JSON\", pas\u00e1s un modelo Pydantic como par\u00e1metro <code>response_format</code>. La API garantiza que la salida conforma ese esquema \u2014 validada antes de que llegue a tu c\u00f3digo.</p> <pre><code>from pydantic import BaseModel\nfrom typing import List, Optional\n\nclass Experience(BaseModel):\n    company: str\n    location: Optional[str]\n    role: str\n    start_date: Optional[str]\n    end_date: Optional[str]\n    responsibilities: Optional[List[str]]\n\nclass Curriculum(BaseModel):\n    full_name: str\n    email: str\n    phone: Optional[str]\n    summary: Optional[str]\n    experience: List[Experience]\n    education: Optional[List[Education]]\n    skills: Optional[List[Skill]]\n    languages: Optional[List[Language]]\n    certifications: Optional[List[str]]\n</code></pre> <p>La llamada a la API es directa:</p> <pre><code>from openai import OpenAI\n\nclient = OpenAI()\n\nresponse = client.beta.chat.completions.parse(\n    model=\"gpt-4o-mini-2024-07-18\",\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": f\"Extract all candidate data from this CV:\\n\\n{markdown_text}\"\n        }\n    ],\n    temperature=0,\n    max_tokens=15000,\n    response_format=Curriculum,\n)\n\ndata = response.choices[0].message.parsed\n</code></pre> <p><code>temperature=0</code> es deliberado \u2014 esto es extracci\u00f3n, no generaci\u00f3n. La salida determin\u00edstica es el objetivo. El modelo se comporta como un operador de carga de datos inteligente: lee el documento y completa los campos del esquema, nada m\u00e1s.</p>"},{"location":"blog/2026/02/11/eliminando-la-carga-manual-de-datos-desde-pdfs-con-ia-un-pipeline-productivo/#cache-por-que-importa-mas-de-lo-que-pensarias","title":"Cach\u00e9: Por Qu\u00e9 Importa M\u00e1s de Lo Que Pensar\u00edas","text":"<p>Un pipeline que reprocesa cada documento en cada ejecuci\u00f3n no es un pipeline \u2014 es un script que te da miedo ejecutar dos veces. El mecanismo de cach\u00e9 usa hashes SHA-256 del contenido de los archivos almacenados en un archivo plano <code>.hashes.txt</code>.</p> <pre><code>import hashlib\n\ndef calculate_file_hash(file_path: str) -&gt; str:\n    hasher = hashlib.sha256()\n    with open(file_path, \"rb\") as f:\n        for chunk in iter(lambda: f.read(8192), b\"\"):\n            hasher.update(chunk)\n    return hasher.hexdigest()\n\ndef is_already_processed(file_path: str) -&gt; bool:\n    file_hash = calculate_file_hash(file_path)\n    with open(\".hashes.txt\", \"r\") as f:\n        existing = f.read().splitlines()\n    return file_hash in existing\n</code></pre> <p>La lectura chunked en modo binario mantiene el uso de memoria constante sin importar el tama\u00f1o del archivo. El chequeo de hash cuesta microsegundos y ahorra una llamada a la API que cuesta dinero real. Con <code>gpt-4o-mini</code>, cada CV cuesta aproximadamente $0.002 \u2014 insignificante por documento, pero no si est\u00e1s reprocesando 500 archivos cada ma\u00f1ana.</p>"},{"location":"blog/2026/02/11/eliminando-la-carga-manual-de-datos-desde-pdfs-con-ia-un-pipeline-productivo/#el-mismo-patron-se-extiende-a-facturas","title":"El Mismo Patr\u00f3n Se Extiende a Facturas","text":"<p>Una de las se\u00f1ales m\u00e1s claras de que una arquitectura es s\u00f3lida es cu\u00e1n f\u00e1cilmente acomoda un segundo caso de uso sin cambios estructurales. Despu\u00e9s de validar el pipeline de CVs, apliqu\u00e9 el mismo patr\u00f3n a la extracci\u00f3n de facturas \u2014 un tipo de documento completamente diferente con campos distintos (CUIT, CAE, l\u00edneas de \u00edtems, desglose de impuestos).</p> <p>El \u00fanico cambio fue el modelo Pydantic. El pipeline \u2014 descubrimiento, hashing, conversi\u00f3n Docling, llamada a OpenAI, salida Excel \u2014 fue id\u00e9ntico. Este es el valor de separar la definici\u00f3n del esquema de la l\u00f3gica de extracci\u00f3n.</p>"},{"location":"blog/2026/02/11/eliminando-la-carga-manual-de-datos-desde-pdfs-con-ia-un-pipeline-productivo/#resultados","title":"Resultados","text":"<p>El pipeline se ejecuta diariamente via cron. Una carpeta de CVs que antes requer\u00eda una ma\u00f1ana dedicada de carga manual de datos ahora produce un libro Excel completamente estructurado y multi-hoja \u2014 candidatos, historial laboral, educaci\u00f3n, habilidades, certificaciones \u2014 cada uno en su propia hoja normalizada, con una clave for\u00e1nea vinculando de vuelta al registro del candidato.</p> <p>El tiempo del equipo de RRHH pas\u00f3 de transcripci\u00f3n a an\u00e1lisis. Ese es el resultado real que vale la pena medir.</p>"},{"location":"blog/2026/02/11/eliminando-la-carga-manual-de-datos-desde-pdfs-con-ia-un-pipeline-productivo/#conclusiones-clave","title":"Conclusiones Clave","text":"<p>La combinaci\u00f3n de Docling, OpenAI Structured Outputs y Pydantic no es solo un stack conveniente \u2014 es una decisi\u00f3n de arquitectura. Cada componente tiene un rol espec\u00edfico:</p> <ul> <li>Docling maneja la impredecibilidad de los formatos PDF antes de que el LLM vea el documento</li> <li>Pydantic define el contrato al que tus datos deben conformarse</li> <li>Structured Outputs aplica ese contrato a nivel de API, no en post-procesamiento</li> <li>Cach\u00e9 SHA-256 hace que el pipeline sea seguro de ejecutar repetidamente sin desperdicio</li> </ul> <p>Cualquiera de estas piezas aislada es \u00fatil. Juntas, producen algo que realmente pod\u00e9s correr en producci\u00f3n.</p>"},{"location":"blog/2026/02/11/eliminando-la-carga-manual-de-datos-desde-pdfs-con-ia-un-pipeline-productivo/#explora-el-proyecto","title":"Explor\u00e1 el Proyecto","text":"<p>La implementaci\u00f3n completa \u2014 incluyendo el notebook de extracci\u00f3n de facturas y el c\u00f3digo de formateo Excel \u2014 est\u00e1 disponible en GitHub:</p> <p>github.com/Joaquin-Urruti/openai-structured-outputs</p> <p>Si est\u00e1s evaluando si este tipo de pipeline se ajusta a un problema de procesamiento de documentos en tu organizaci\u00f3n, estoy feliz de conversarlo. Pod\u00e9s contactarme via LinkedIn o agendar una llamada directamente.</p> <p>Agendar Llamada Introductoria Gratuita </p>"},{"location":"portfolio/","title":"Proyectos Destacados","text":"<p>Bienvenido a mi portfolio de proyectos de ciencia de datos geoespaciales e IA. Cada proyecto demuestra mi experiencia en entregar soluciones impactantes a desaf\u00edos de negocio del mundo real.</p> <ul> <li> <p>Mapeo autom\u00e1tico de cumplimiento de aplicaciones fitosanitarias</p> <p>Sistema GIS automatizado para calcular zonas de restricci\u00f3n de aplicaciones fitosanitarias en campos agr\u00edcolas argentinos, procesando regulaciones multi-jurisdiccionales para generar mapas de cumplimiento e informes para operaciones de pulverizaci\u00f3n a\u00e9rea y terrestre.</p> </li> <li> <p>Matrices de Distancia a Gran Escala con Servidor OSRM Local</p> <p>Ejecutar OSRM localmente permite el c\u00e1lculo r\u00e1pido e ilimitado de grandes matrices de distancia origen-destino usando redes viales reales. Este enfoque elimina los l\u00edmites de APIs p\u00fablicas y ayuda a las empresas a modelar costos log\u00edsticos con precisi\u00f3n calculando kil\u00f3metros entre miles de or\u00edgenes y destinos.</p> </li> <li> <p>Despliegue de Plataforma GeoNode para Presentaci\u00f3n Profesional GIS</p> <p>Implementaci\u00f3n de una plataforma web geoespacial basada en GeoNode para publicar y compartir proyectos cartogr\u00e1ficos profesionales. El despliegue en un servidor dedicado usando Docker Compose resolvi\u00f3 conflictos t\u00e9cnicos de puertos y health checks, permitiendo controlar y personalizar por completo la presentaci\u00f3n de trabajos GIS para clientes y organizaciones, con acceso seguro y alta disponibilidad.</p> </li> </ul>"},{"location":"portfolio/projects/project-1/","title":"Mapeo autom\u00e1tico de cumplimiento de aplicaciones fitosanitarias en Argentina","text":"<p>Resumen del Caso de Estudio</p> <p>Cliente: Empresa Argentina de Producci\u00f3n Agr\u00edcola  Industria: Tecnolog\u00eda Agr\u00edcola / Cumplimiento Regulatorio</p> <p>M\u00e9tricas de Impacto:</p> <ul> <li>Reducci\u00f3n de 1000x en tiempo de procesamiento GIS manual por campa\u00f1a</li> <li>+2100 lotes agr\u00edcolas analizados en toda Argentina por ciclo de procesamiento</li> <li>+70 departamentos/partidos con legislaci\u00f3n rastreada en la base de datos de cumplimiento</li> <li>100% de cumplimiento con regulaciones fitosanitarias locales en todas las jurisdicciones</li> <li>+100 horas de analista ahorradas por campa\u00f1a agr\u00edcola</li> </ul>"},{"location":"portfolio/projects/project-1/#automatizacion-completa-del-mapeo-de-cumplimiento-de-aplicaciones-fitosanitarias-en-argentina-con-un-flujo-de-trabajo-gis-automatizado","title":"Automatizaci\u00f3n completa del mapeo de cumplimiento de aplicaciones fitosanitarias en Argentina con un flujo de trabajo GIS automatizado","text":""},{"location":"portfolio/projects/project-1/#resumen","title":"Resumen","text":"<p>Desarrollo de un sistema de cumplimiento geoespacial automatizado que calcula zonas de exclusi\u00f3n y amortiguaci\u00f3n para aplicaciones de productos fitosanitarios en campos agr\u00edcolas de Argentina. El sistema procesa m\u00faltiples marcos regulatorios de diferentes jurisdicciones para generar mapas de restricci\u00f3n precisos e informes de cumplimiento tanto para aplicaciones a\u00e9reas como terrestres.</p>"},{"location":"portfolio/projects/project-1/#el-desafio","title":"El Desaf\u00edo","text":"<p>Las operaciones agr\u00edcolas en Argentina deben cumplir con regulaciones estrictas respecto a la aplicaci\u00f3n de productos fitosanitarios cerca de \u00e1reas sensibles como escuelas, zonas urbanas, cuerpos de agua y cortinas de \u00e1rboles. La complejidad surge de tres factores clave:</p> <p>Fragmentaci\u00f3n Jurisdiccional: Cada provincia y departamento (o partido) en Argentina tiene autoridad aut\u00f3noma para definir sus propias distancias de restricci\u00f3n, resultando en un mosaico de regulaciones a lo largo de las regiones agr\u00edcolas del pa\u00eds.</p> <p>M\u00faltiples Tipos de Restricci\u00f3n: Las regulaciones distinguen entre dos m\u00e9todos de aplicaci\u00f3n (a\u00e9reo y terrestre) y dos niveles de restricci\u00f3n (zonas de exclusi\u00f3n total donde no se permite ninguna aplicaci\u00f3n, y zonas de amortiguaci\u00f3n donde solo se permiten productos de banda verde).</p> <p>Escala Operacional: La empresa gestiona campos agr\u00edcolas distribuidos a lo largo de todo el cintur\u00f3n agr\u00edcola argentino, requiriendo verificaci\u00f3n de cumplimiento para docenas de jurisdicciones con diferentes requisitos regulatorios cada temporada de cultivo.</p> <p>Anteriormente, este an\u00e1lisis requer\u00eda mucho trabajo GIS manual para cada departamento\u2014creando buffers individuales alrededor de objetos sensibles usando distancias espec\u00edficas de la jurisdicci\u00f3n, luego intersectando estos con los l\u00edmites del campo. Este proceso era lento, propenso a errores y dif\u00edcil de delegar a especialistas no-GIS.</p>"},{"location":"portfolio/projects/project-1/#enfoque-tecnico","title":"Enfoque T\u00e9cnico","text":""},{"location":"portfolio/projects/project-1/#stack-tecnologico","title":"Stack Tecnol\u00f3gico","text":"<ul> <li>Lenguaje de Programaci\u00f3n: Python 3.x</li> <li>Procesamiento Geoespacial: GeoPandas, Shapely</li> <li>Entorno de Desarrollo: Google Colab (Jupyter Notebook)</li> <li>Almacenamiento de Datos: Google Drive</li> <li>Formatos de Datos: GeoPackage (.gpkg), Excel (.xlsx)</li> <li>Sistema de Referencia de Coordenadas: EPSG:32720 (UTM Zona 20S)</li> </ul>"},{"location":"portfolio/projects/project-1/#arquitectura","title":"Arquitectura","text":"<p>Arquitectura del Sistema</p> <p>La soluci\u00f3n sigue una arquitectura de procesamiento por lotes con almacenamiento y ejecuci\u00f3n basados en la nube.</p> <p>Componentes:</p> <ul> <li>Capa de Entrada: Carpeta de Google Drive conteniendo archivos GeoPackage estandarizados</li> <li>Motor de Procesamiento: Notebook Python ejecutando operaciones espaciales</li> <li>Base de Datos de Legislaci\u00f3n: GeoPackage con matriz regulatoria por jurisdicci\u00f3n</li> <li>Generador de Salida: Exportaci\u00f3n automatizada de capas procesadas e informes</li> </ul>"},{"location":"portfolio/projects/project-1/#modelo-de-datos","title":"Modelo de Datos","text":"<p>La base de datos de legislaci\u00f3n mantiene una matriz regulatoria con 15 par\u00e1metros de distancia por departamento:</p> Tipo de Par\u00e1metro Objetos Cubiertos Tipos de Aplicaci\u00f3n Distancias de exclusi\u00f3n \u00c1rboles, \u00c1reas urbanas, Escuelas, Cursos de agua, Cuerpos de agua Terrestre, A\u00e9reo Distancias de amortiguaci\u00f3n \u00c1rboles, \u00c1reas urbanas, Escuelas, Cursos de agua, Cuerpos de agua Terrestre, A\u00e9reo"},{"location":"portfolio/projects/project-1/#aspectos-destacados-de-la-implementacion","title":"Aspectos Destacados de la Implementaci\u00f3n","text":""},{"location":"portfolio/projects/project-1/#gestion-de-la-matriz-regulatoria","title":"Gesti\u00f3n de la Matriz Regulatoria","text":"<p>El sistema ingiere una capa de legislaci\u00f3n donde cada departamento contiene valores de distancia codificados siguiendo una convenci\u00f3n de nomenclatura estandarizada:</p> <pre><code>DISTANCE_CODES = {\n    'EPT': 'Exclusion - Urban Areas - Terrestrial',\n    'APT': 'Buffer - Urban Areas - Terrestrial',\n    'EPA': 'Exclusion - Urban Areas - Aerial',\n    'APA': 'Buffer - Urban Areas - Aerial',\n    'EET': 'Exclusion - Schools - Terrestrial',\n    'AET': 'Buffer - Schools - Terrestrial',\n    # C\u00f3digos adicionales para cuerpos de agua y cursos de agua\n}\n</code></pre>"},{"location":"portfolio/projects/project-1/#generacion-dinamica-de-buffers","title":"Generaci\u00f3n Din\u00e1mica de Buffers","text":"<p>Un desaf\u00edo t\u00e9cnico clave fue manejar la naturaleza acumulativa de las zonas de amortiguaci\u00f3n. La distancia de amortiguaci\u00f3n se mide desde el objeto sensible, no desde el l\u00edmite de la zona de exclusi\u00f3n. La soluci\u00f3n ajusta las distancias de buffer autom\u00e1ticamente.</p>"},{"location":"portfolio/projects/project-1/#pipeline-de-procesamiento-espacial","title":"Pipeline de Procesamiento Espacial","text":"<p>El procesamiento central sigue un enfoque sistem\u00e1tico:</p> <ol> <li>Reparaci\u00f3n de Geometr\u00eda: Las capas de entrada se limpian para corregir errores de geometr\u00eda y topolog\u00eda</li> <li>Reproyecci\u00f3n: Todas las capas convertidas a UTM Zona 20S para c\u00e1lculos m\u00e9tricos</li> <li>Uni\u00f3n Espacial: Los objetos sensibles heredan par\u00e1metros regulatorios de su departamento contenedor</li> <li>Generaci\u00f3n de Buffers: 15 capas de buffer distintas creadas basadas en tipo de objeto y categor\u00eda de restricci\u00f3n</li> <li>Intersecci\u00f3n: Buffers recortados a los l\u00edmites del lote agr\u00edcola</li> <li>Resoluci\u00f3n de Superposici\u00f3n: A las zonas de amortiguaci\u00f3n se les resta las zonas de exclusi\u00f3n para prevenir doble conteo</li> </ol> <p></p> <p>Diagrama del flujo de trabajo de procesamiento geoespacial para zonas de exclusi\u00f3n y amortiguaci\u00f3n: desde la ingesti\u00f3n de datos regulatorios y geogr\u00e1ficos hasta la generaci\u00f3n de buffers no superpuestos por tipo de restricci\u00f3n, recortados a los l\u00edmites de parcelas agr\u00edcolas.</p> <p> Distintas zonas de exclusi\u00f3n y amortiguaci\u00f3n para aplicaciones terrestres y a\u00e9reas en distintos campos.</p>"},{"location":"portfolio/projects/project-1/#resultados-e-impacto","title":"Resultados e Impacto","text":"<p>El sistema automatizado entrega mejoras operacionales significativas:</p> M\u00e9trica Resultado Reducci\u00f3n de tiempo de procesamiento 99% comparado con flujo de trabajo manual - de 100 horas a menos de 10 minutos Jurisdicciones cubiertas +300 departamentos en toda Argentina Precisi\u00f3n 100% cumplimiento regulatorio con ordenanzas locales Generaci\u00f3n de informes Salidas automatizadas en Excel y GeoPackage Accesibilidad de usuarios Especialistas no-GIS pueden ejecutar el flujo de trabajo <p>Beneficios de Negocio:</p> <ul> <li>Negociaci\u00f3n Pre-arrendamiento: Datos de restricci\u00f3n disponibles antes de decisiones de alquiler de campos, habilitando negociaciones informadas</li> <li>Cumplimiento Regulatorio: Documentaci\u00f3n completa de \u00e1reas restringidas para prop\u00f3sitos de auditor\u00eda</li> <li>An\u00e1lisis Estrat\u00e9gico: M\u00e9tricas agregadas por zona, departamento o tipo de restricci\u00f3n para reportes gerenciales</li> <li>Planificaci\u00f3n Operacional: Delineaci\u00f3n clara de \u00e1reas que requieren tratamiento especial o restricciones de productos</li> </ul>"},{"location":"portfolio/projects/project-1/#mis-contribuciones","title":"Mis Contribuciones","text":"<p>Como \u00fanico desarrollador de esta soluci\u00f3n, mis responsabilidades incluyeron:</p> <ul> <li>An\u00e1lisis de Requerimientos: Traduje requerimientos regulatorios complejos en un modelo de datos estructurado</li> <li>Dise\u00f1o de Arquitectura: Dise\u00f1\u00e9 el flujo de trabajo basado en la nube optimizado para delegaci\u00f3n a usuarios no t\u00e9cnicos</li> <li>Desarrollo Geoespacial: Implement\u00e9 todos los algoritmos de procesamiento espacial incluyendo generaci\u00f3n de buffers, intersecci\u00f3n y resoluci\u00f3n de superposici\u00f3n</li> <li>Documentaci\u00f3n: Produje documentaci\u00f3n t\u00e9cnica comprensiva para mantenimiento y transferencia de conocimiento</li> </ul>"},{"location":"portfolio/projects/project-1/#lecciones-aprendidas","title":"Lecciones Aprendidas","text":"<p>La Simplicidad Habilita la Adopci\u00f3n: Al dise\u00f1ar la soluci\u00f3n como un notebook de Colab con integraci\u00f3n de Google Drive, el flujo de trabajo pudo ser delegado a miembros del equipo sin experiencia en GIS. La decisi\u00f3n de priorizar la usabilidad sobre la sofisticaci\u00f3n t\u00e9cnica demostr\u00f3 ser cr\u00edtica para el \u00e9xito operacional.</p> <p>Mantenimiento de Datos Regulatorios: El aspecto m\u00e1s desafiante es mantener la base de datos de legislaci\u00f3n actualizada. Establecer un proceso claro para monitorear cambios regulatorios y actualizar la base de datos es tan importante como la implementaci\u00f3n t\u00e9cnica.</p> <p>La Precisi\u00f3n Geom\u00e9trica Importa: El cumplimiento agr\u00edcola requiere c\u00e1lculos de \u00e1rea precisos. Invertir tiempo en reparaci\u00f3n de geometr\u00eda y selecci\u00f3n adecuada del sistema de coordenadas previno problemas posteriores con discrepancias de \u00e1rea.</p> <ul> <li> <p> \u00a1Tomemos un caf\u00e9 virtual juntos!</p> <p>\u00bfQuer\u00e9s ver si somos un buen match? Charlemos y descubr\u00e1moslo. Agend\u00e1 una sesi\u00f3n de estrategia gratuita de 30 minutos para discutir tus desaf\u00edos y explorar c\u00f3mo podemos trabajar juntos.</p> <p>Agend\u00e1 una llamada gratuita </p> </li> </ul>"},{"location":"portfolio/projects/project-2/","title":"C\u00e1lculo de Matrices de Distancia a Gran Escala con un Servidor OSRM Local","text":"<p>Resumen del Proyecto</p> <p>Tipo de Proyecto: Herramienta Open Source / Infraestructura Interna Repositorio: osrm-local-server Industria: Log\u00edstica / Cadena de Suministro / Distribuci\u00f3n Agr\u00edcola</p> <p>Resultados Clave:</p> <ul> <li>C\u00e1lculos de distancia ilimitados sin l\u00edmites de tasa de API</li> <li>Generaci\u00f3n muy r\u00e1pida de matrices para grandes pares origen-destino</li> <li>100% de eliminaci\u00f3n de costos vs. uso de APIs de ruteo comerciales</li> <li>Control total sobre la frescura de datos y par\u00e1metros de ruteo</li> <li>M\u00e9tricas de distancia repetibles y auditables para cumplimiento</li> <li>Escalable a redes log\u00edsticas regionales o a nivel pa\u00eds</li> </ul> <p>Este proyecto aborda un cuello de botella com\u00fan en operaciones log\u00edsticas: calcular distancias por carretera entre muchas ubicaciones eficientemente. Cuando las APIs p\u00fablicas de ruteo se vuelven muy lentas, costosas o limitadas en tasa, ejecutar OSRM localmente transforma el ruteo en un servicio predecible y de alto rendimiento que escala con las necesidades de tu negocio.</p>"},{"location":"portfolio/projects/project-2/#necesitas-calcular-costos-de-transporte-basados-en-una-matriz-de-distancia-muy-grande","title":"\u00bfNecesit\u00e1s calcular costos de transporte basados en una matriz de distancia muy grande?","text":"<p>Cuando est\u00e1s calculando precios de log\u00edstica, los kil\u00f3metros son dinero. Si tu negocio necesita computar distancias por carretera entre muchos or\u00edgenes (almacenes, campos, tiendas, ubicaciones de proveedores) y muchos destinos (clientes, puertos, plantas, centros de distribuci\u00f3n), el enfoque usual de \"llamar a una API de ruteo por par\" colapsa r\u00e1pidamente: es lento, tiene l\u00edmites de tasa y es costoso.</p> <p>Este proyecto proporciona una alternativa pr\u00e1ctica: ejecutar OSRM localmente y generar grandes matrices de distancia origen-destino (OD) r\u00e1pidamente, usando el Table Service de OSRM en lugar de llamadas de ruta individuales.</p> <p>El resultado es un sistema dise\u00f1ado para empresas que necesitan calcular costos log\u00edsticos a escala, sin depender de la API p\u00fablica de OSRM o sus l\u00edmites de uso.</p>"},{"location":"portfolio/projects/project-2/#por-que-esto-importa-para-el-modelado-de-costos","title":"Por qu\u00e9 esto importa para el modelado de costos","text":"<p>En la mayor\u00eda de los modelos de log\u00edstica y cadena de suministro, la distancia es un input central:</p> <ul> <li>El costo de transporte es usualmente proporcional a los kil\u00f3metros recorridos.</li> <li>Elegir el destino \u00f3ptimo (planta, puerto, CD) depende de las distancias relativas.</li> <li>Los costos deben recalcularse frecuentemente a medida que cambian los vol\u00famenes, rutas o condiciones comerciales.</li> </ul> <p>Las APIs p\u00fablicas de ruteo no est\u00e1n dise\u00f1adas para estas cargas de trabajo. Los l\u00edmites de tasa, topes de solicitudes y latencia las hacen inadecuadas para construir grandes matrices OD repetidamente.</p> <p>Ejecutar OSRM localmente elimina estas restricciones y convierte el ruteo en un servicio interno predecible y de alto rendimiento.</p>"},{"location":"portfolio/projects/project-2/#lo-que-entrega-el-proyecto","title":"Lo que entrega el proyecto","text":"<p>Este repositorio implementa un flujo de trabajo de extremo a extremo para computar matrices de distancia usando una instancia local de OSRM:</p> <ol> <li>Iniciar un servidor OSRM localmente usando Docker y datos de OpenStreetMap.</li> <li>Cargar geometr\u00edas de origen (pol\u00edgonos) y puntos de destino desde archivos GIS.</li> <li>Generar centroides para los pol\u00edgonos de origen.</li> <li>Ajustar los centroides al segmento de carretera m\u00e1s cercano para asegurar puntos ruteables.</li> <li>Usar el Table Service de OSRM para computar todas las distancias y duraciones origen-destino en una sola solicitud.</li> <li>Exportar los resultados a Excel para an\u00e1lisis adicional o modelado de costos.</li> </ol> <p>La configuraci\u00f3n de ejemplo est\u00e1 preparada para Argentina, pero el mismo enfoque funciona para cualquier regi\u00f3n con extractos disponibles de OpenStreetMap.</p>"},{"location":"portfolio/projects/project-2/#decisiones-arquitectonicas-clave","title":"Decisiones arquitect\u00f3nicas clave","text":""},{"location":"portfolio/projects/project-2/#osrm-ejecutandose-localmente-en-docker","title":"OSRM ejecut\u00e1ndose localmente en Docker","text":"<p>OSRM es un motor de ruteo de alto rendimiento construido sobre datos de OpenStreetMap. Ejecutarlo localmente da control total sobre:</p> <ul> <li>Frescura de datos</li> <li>Recursos de c\u00f3mputo</li> <li>Volumen de solicitudes</li> <li>Reproducibilidad de resultados</li> </ul> <p>Docker asegura que la configuraci\u00f3n sea repetible y f\u00e1cil de desplegar en diferentes entornos.</p>"},{"location":"portfolio/projects/project-2/#usando-el-table-service-para-matrices","title":"Usando el Table Service para matrices","text":"<p>En lugar de computar una ruta a la vez, el proyecto usa el Table Service de OSRM, que computa una matriz completa de distancias y duraciones entre m\u00faltiples or\u00edgenes y destinos en una sola llamada.</p> <p>Este enfoque reduce dr\u00e1sticamente la sobrecarga y hace factibles las matrices grandes.</p>"},{"location":"portfolio/projects/project-2/#ajustando-origenes-a-la-red-vial","title":"Ajustando or\u00edgenes a la red vial","text":"<p>Los datos de origen a menudo vienen como pol\u00edgonos (campos, zonas, \u00e1reas de servicio). Sus centroides pueden no estar exactamente sobre una carretera ruteable.</p> <p>El pipeline ajusta cada centroide al nodo de carretera m\u00e1s cercano antes del ruteo, asegurando c\u00e1lculos de distancia realistas y confiables.</p> <p> Esquema del flujo de trabajo</p>"},{"location":"portfolio/projects/project-2/#como-ejecutar-el-proyecto","title":"C\u00f3mo ejecutar el proyecto","text":""},{"location":"portfolio/projects/project-2/#requisitos","title":"Requisitos","text":"<ul> <li>Docker</li> <li>Python 3.11+</li> <li>Jupyter Notebook</li> <li>Dependencias instaladas v\u00eda <code>uv</code> o <code>pip</code></li> </ul>"},{"location":"portfolio/projects/project-2/#iniciar-el-servidor-osrm-local","title":"Iniciar el servidor OSRM local","text":"<pre><code>git clone https://github.com/Joaquin-Urruti/osrm-local-server\ncd osrm-local-server\n\nuv sync\n# o\npip install -r requirements.txt\n\n./iniciar_server_osrm_docker.sh\n</code></pre> <p>La primera ejecuci\u00f3n preprocesa el extracto de OpenStreetMap y toma m\u00e1s tiempo. Las ejecuciones posteriores reutilizan los datos procesados e inician r\u00e1pidamente.</p>"},{"location":"portfolio/projects/project-2/#datos-de-entrada","title":"Datos de entrada","text":""},{"location":"portfolio/projects/project-2/#coloca-tus-archivos-gis-bajo-inputs","title":"Coloc\u00e1 tus archivos GIS bajo inputs:","text":"<ul> <li>\"origins.gpkg\" o .shp: geometr\u00edas de pol\u00edgonos con identificadores (en este proyecto se usaron pol\u00edgonos, pero podr\u00edan ser puntos).</li> <li>\"destinations.gpkg\" o .shp: geometr\u00edas de puntos para destinos.</li> </ul> <p>Todos los datos se convierten internamente a WGS84 (EPSG:4326) para compatibilidad con OSRM.</p>"},{"location":"portfolio/projects/project-2/#generar-la-matriz-de-distancias","title":"Generar la matriz de distancias","text":""},{"location":"portfolio/projects/project-2/#ejecuta-el-notebook-distancias_tableipynb-este","title":"Ejecut\u00e1 el notebook distancias_table.ipynb. Este:","text":"<ul> <li>Filtra or\u00edgenes por campa\u00f1a y zonas opcionales</li> <li>Genera y ajusta centroides</li> <li>Consulta el Table Service de OSRM local</li> <li>Exporta la matriz OD a: outputs/matrix.xlsx</li> </ul> <p>Cada fila representa un par origen-destino con distancia (km) y duraci\u00f3n (horas).</p>"},{"location":"portfolio/projects/project-2/#resultados-e-impacto-de-negocio","title":"Resultados e impacto de negocio","text":"<p>El resultado principal es la capacidad de computar grandes matrices de distancia r\u00e1pida y confiablemente, sin depender del servidor p\u00fablico de OSRM.</p>"},{"location":"portfolio/projects/project-2/#para-empresas-esto-habilita","title":"Para empresas, esto habilita:","text":"<ul> <li>C\u00e1lculos de costos log\u00edsticos m\u00e1s r\u00e1pidos</li> <li>M\u00e9tricas de distancia repetibles y auditables</li> <li>An\u00e1lisis de escenarios a trav\u00e9s de muchos or\u00edgenes y destinos</li> <li>Control total sobre datos de ruteo y rendimiento</li> </ul> <p>Esta configuraci\u00f3n es particularmente valiosa para log\u00edstica agr\u00edcola, distribuci\u00f3n retail, cadenas de suministro de manufactura, y cualquier operaci\u00f3n donde los kil\u00f3metros afectan directamente los m\u00e1rgenes.</p>"},{"location":"portfolio/projects/project-2/#en-resumen","title":"En resumen","text":"<p>Si tu organizaci\u00f3n necesita calcular costos log\u00edsticos a escala y las APIs p\u00fablicas de ruteo se est\u00e1n convirtiendo en un cuello de botella, ejecutar OSRM localmente con el Table Service es un enfoque s\u00f3lido y listo para producci\u00f3n.</p> <p>Este proyecto proporciona un punto de partida concreto y reutilizable para construir esas matrices de distancia eficientemente y sin l\u00edmites externos.</p> <ul> <li> <p> \u00a1Tomemos un caf\u00e9 virtual juntos!</p> <p>\u00bfQuer\u00e9s ver si somos un buen match? Charlemos y descubr\u00e1moslo. Agend\u00e1 una sesi\u00f3n de estrategia gratuita de 30 minutos para discutir tus desaf\u00edos y explorar c\u00f3mo podemos trabajar juntos.</p> <p>Agend\u00e1 una llamada gratuita </p> </li> </ul>"},{"location":"portfolio/projects/project-3/","title":"Despliegue de Plataforma Geoespacial para Presentaci\u00f3n Profesional de Trabajos GIS","text":"<p>Resumen del Caso de Estudio</p> <p>Cliente: Profesional GIS Independiente Industria: Tecnolog\u00eda Geoespacial / Open Source</p> <p>M\u00e9tricas de Impacto:</p> <ul> <li>Plataforma desplegada y funcionando en producci\u00f3n</li> <li>7 contenedores Docker orquestados exitosamente</li> <li>2 problemas t\u00e9cnicos cr\u00edticos resueltos durante la implementaci\u00f3n</li> <li>Tiempo de implementaci\u00f3n: ~2 semanas</li> <li>99% uptime mensual objetivo</li> </ul>"},{"location":"portfolio/projects/project-3/#resumen","title":"Resumen","text":"<p>GeoNode es una plataforma web geoespacial open source que puede ser desplegada para un profesional GIS independiente o cualquier organizaci\u00f3n que necesite publicar mapas, tableros o contenido web basado en capas geoespaciales. El proyecto consisti\u00f3 en desplegar GeoNode en un servidor dedicado utilizando Docker Compose, enfrentando y resolviendo desaf\u00edos t\u00e9cnicos relacionados con conflictos de puertos y configuraci\u00f3n de health checks. La plataforma proporciona un punto centralizado para la publicaci\u00f3n de proyectos profesionales con acceso para clientes y control total para el editor.</p>"},{"location":"portfolio/projects/project-3/#el-desafio","title":"El Desaf\u00edo","text":"<p>Como profesional independiente, decid\u00ed montar una plataforma propia para publicar y compartir proyectos cartogr\u00e1ficos con una experiencia simple para el cliente y con margen real de crecimiento. Los factores clave de la decisi\u00f3n fueron:</p> <p>Presupuesto limitado: Servidor dedicado existente, sin recursos para soluciones gestionadas.</p> <p>Necesidad de control total: Quer\u00eda una plataforma propia, no servicios externos como ArcGIS Online. Plataforma propia con control total sobre datos, permisos, actualizaciones y backups.</p> <p>Accesibilidad para clientes: Los clientes deben poder ver los proyectos sin capacitaci\u00f3n t\u00e9cnica, con visualizaci\u00f3n en navegador, sin capacitaci\u00f3n.</p> <p>Flexibilidad para crecer: Cat\u00e1logo, metadatos, roles, colaboraci\u00f3n, estilos.</p> <p>Integraci\u00f3n con stack GIS: Est\u00e1ndares e interoperabilidad (OGC).</p> <p>Base reutilizable para futuros proyectos: Plantillas y flujo repetible.</p>"},{"location":"portfolio/projects/project-3/#requisitos-tecnicos","title":"Requisitos T\u00e9cnicos","text":"Requisito Descripci\u00f3n Plataforma GIS web GeoNode como base Contenedores Docker Facilitar despliegue y mantenimiento GeoServer incluido Servicios OGC (WMS, WFS) HTTPS Certificados v\u00e1lidos Acceso p\u00fablico Posibilidad de publicar mapas con/sin autenticaci\u00f3n"},{"location":"portfolio/projects/project-3/#criterios-de-exito","title":"Criterios de \u00c9xito","text":"<ul> <li>Plataforma operativa con uptime del 99%</li> <li>GeoServer accesible para administraci\u00f3n</li> <li>Capas p\u00fablicas visibles sin login</li> <li>Mantenimiento gestionable por el propietario</li> </ul>"},{"location":"portfolio/projects/project-3/#enfoque-tecnico","title":"Enfoque T\u00e9cnico","text":""},{"location":"portfolio/projects/project-3/#stack-tecnologico-arquitectura","title":"Stack Tecnol\u00f3gico &amp; Arquitectura","text":"<p>Arquitectura del Sistema</p> <p>La soluci\u00f3n implementa una arquitectura de microservicios con 7 contenedores Docker:</p> <p>Componentes:</p> <ul> <li>nginx: Reverse Proxy (puertos 80, 443)</li> <li>Kong: API Gateway (puerto 8001)</li> <li>Django: Aplicaci\u00f3n web GeoNode (puerto 8000)</li> <li>GeoServer: Servidor de mapas OGC (puerto 8083)</li> <li>PostgreSQL/PostGIS: Base de datos espacial (puerto 5432)</li> <li>Redis: Cach\u00e9 y broker de mensajes (puerto 6379)</li> <li>Memcached: Cach\u00e9 adicional (puerto 11211)</li> <li>Celery: Procesamiento de tareas en background</li> </ul> <p>Flujo de datos: nginx/Kong \u2192 Django \u2192 GeoServer/PostgreSQL</p> <p></p> <p>Arquitectura de contenedores</p>"},{"location":"portfolio/projects/project-3/#aspectos-destacados-de-la-implementacion","title":"Aspectos Destacados de la Implementaci\u00f3n","text":""},{"location":"portfolio/projects/project-3/#fases-del-proyecto","title":"Fases del Proyecto","text":"<p>Fase 1: Preparaci\u00f3n - Configuraci\u00f3n inicial del servidor - Instalaci\u00f3n de Docker y Docker Compose - Clonaci\u00f3n del repositorio GeoNode - Configuraci\u00f3n de archivo <code>.env</code></p> <p>Fase 2: Despliegue Inicial - Levantamiento de contenedores base - Ejecuci\u00f3n de migraciones de base de datos - Configuraci\u00f3n de variables de entorno - Creaci\u00f3n de superusuario administrador</p> <p>Fase 3: Resoluci\u00f3n de Problemas - Identificaci\u00f3n y resoluci\u00f3n de conflictos de puerto - Ajuste de health checks - Verificaci\u00f3n de conectividad entre servicios</p> <p>Fase 4: Producci\u00f3n - Configuraci\u00f3n de dominios y DNS - Implementaci\u00f3n de HTTPS - Pruebas finales y validaci\u00f3n</p>"},{"location":"portfolio/projects/project-3/#resultados-e-impacto","title":"Resultados e Impacto","text":""},{"location":"portfolio/projects/project-3/#metricas-tecnicas","title":"M\u00e9tricas T\u00e9cnicas","text":"M\u00e9trica Objetivo Estado Uptime mensual 99% En monitoreo Contenedores activos 7/7 Logrado Puerto GeoServer 8083 Configurado Health check Django Healthy Corregido Tiempo de carga &lt; 5 seg Verificado Usuarios concurrentes 50+ Por verificar"},{"location":"portfolio/projects/project-3/#componentes-desplegados","title":"Componentes Desplegados","text":"Componente Puerto Estado Django (GeoNode) 8000 Healthy GeoServer Admin 8083 Corriendo nginx 80, 443 Corriendo PostgreSQL/PostGIS 5432 Healthy Redis 6379 Corriendo Memcached 11211 Corriendo Kong Admin 8001 Corriendo"},{"location":"portfolio/projects/project-3/#valor-entregado","title":"Valor Entregado","text":"<p>Para el Editor (Propietario): - Control total sobre la plataforma - No dependencia de servicios externos - Costo reducido (solo servidor dedicado) - Flexibilidad para personalizar</p> <p>Para los Clientes: - Acceso sin autenticaci\u00f3n a capas p\u00fablicas - Interfaz intuitiva para visualizaci\u00f3n - Descarga de datos disponible - Compartir URLs directamente</p>"},{"location":"portfolio/projects/project-3/#mis-contribuciones","title":"Mis Contribuciones","text":"<p>Como \u00fanico desarrollador de esta soluci\u00f3n, mis responsabilidades incluyeron:</p> <ul> <li>An\u00e1lisis de Requerimientos: Evaluaci\u00f3n de necesidades t\u00e9cnicas y selecci\u00f3n de tecnolog\u00eda</li> <li>Despliegue e Implementaci\u00f3n: Configuraci\u00f3n y puesta en marcha de todos los componentes</li> <li>Resoluci\u00f3n de Problemas: Identificaci\u00f3n y soluci\u00f3n de conflictos de puertos y health checks</li> <li>Documentaci\u00f3n: Producci\u00f3n de gu\u00edas de gesti\u00f3n y troubleshooting</li> </ul> <ul> <li> <p> \u00a1Tomemos un caf\u00e9 virtual juntos!</p> <p>\u00bfNecesit\u00e1s desplegar tu propia plataforma de datos GIS? Agend\u00e1 una sesi\u00f3n gratuita de 30 minutos para discutir tus desaf\u00edos y explorar c\u00f3mo podemos trabajar juntos.</p> <p>Agend\u00e1 una llamada gratuita </p> </li> </ul>"},{"location":"blog/archive/2026/","title":"2026","text":""},{"location":"blog/category/gis/","title":"GIS","text":""},{"location":"blog/category/data-engineering/","title":"Data Engineering","text":""},{"location":"blog/category/python/","title":"Python","text":""},{"location":"blog/category/xarray/","title":"Xarray","text":""},{"location":"blog/category/ingenier%C3%ADa-de-ia/","title":"Ingenier\u00eda de IA","text":""},{"location":"blog/category/extracci%C3%B3n-de-datos/","title":"Extracci\u00f3n de Datos","text":""},{"location":"blog/category/procesamiento-de-documentos/","title":"Procesamiento de Documentos","text":""},{"location":"blog/category/herramientas/","title":"Herramientas","text":""},{"location":"blog/category/tips-r%C3%A1pidos/","title":"Tips R\u00e1pidos","text":""},{"location":"blog/category/comandos/","title":"Comandos","text":""}]}